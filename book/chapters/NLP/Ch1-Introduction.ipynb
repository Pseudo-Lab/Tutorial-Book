{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Ch1-Introduction.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ptu_FuqCTYYq"
      },
      "source": [
        "# 1. 소개"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ka9ab3KpThBS"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Pseudo-Lab/Tutorial-Book/blob/master/book/chapters/NLP/Ch1-Introduction.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7epAhbI9TpEd"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kgxQtUyNTnkX"
      },
      "source": [
        "## 1.1 문제 생성 작업 (Question Generation Task)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4KiUcXH_LC6p"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V7yydlpETxFe"
      },
      "source": [
        "## 1.2 BERT\n",
        "\n",
        "[BERT](https://github.com/google-research/bert) (Bidirectional Encoder Representations from Transformers)는 2019년 10월 25일 구글 리서치 팀에 의해 공개된 자연어처리 사전 훈련 모델입니다. BERT 모델은 100여개가 넘는 언어 학습을 지원하며, `BERT-Base`, `BERT-Large`, `BERT-Base, Multilingual`, 그리고 `BERT-Base, Chinese` 모델이 있습니다. 각각의 모델 뒤에 `Cased`와 `Uncased`가 붙혀져 있는데, `Uncased`의 경우 대소문자 구분을 하지 않는 모델입니다.\n",
        "\n",
        "BERT를 이용하여 특정 과제를 수행할 수 있는데요, 이를 위해서는 세부적인 과제를 수행하도록 파인튜닝(fine-tuning) 작업이 필요합니다. BERT은 사전 학습된 언어모델로서 파인튜닝을 통해, 원하는 작업을 수행할 수 있습니다. 사전 학습된 언어모델을 이용하여 다운스트림 작업(down-stream tasks)을 수행할 수 있는 방법에는 `feature-based`와 파인튜닝(`fine-tuning`) 방식이 있습니다. `feature-based` 접근법에는 Word2Vec, GloVe, ELMo와 같은 방식이 있으며, 파인튜닝(`fine-tuning`) 방식에는 대표적으로 GPT 모델이 있습니다.\n",
        "\n",
        "BERT가 발표 되었을 당시, GPT-2와 비교를 많이 했는데요, 두 모델 모두 파인튜닝(`fine-tuning`) 방식을 사용하지만, GPT-2는 단방향(unidirectional) 언어 모델인 반면, BERT는 양방향(bidirectional) 언어 모델로서 차이가 있었습니다. 향후 GPT를 개발한 OpenAI에서는 BERT처럼 양방향 언어 모델인 GPT-3를 내놓게 됩니다.\n",
        "\n",
        "양방향 언어 모델의 장점은 `fill-in-the-blanks`와 같이 앞뒤 문맥에 맞게 빈칸에 알맞은 단어를 추측하는 작업에 아주 높은 성능을 보여주고 있습니다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rbDoMr9ZT2fo"
      },
      "source": [
        "## 1.3 RoBERTa"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qZQYcfHKTzTq"
      },
      "source": [
        "## 1.4 GPT-2 (미정)"
      ]
    }
  ]
}