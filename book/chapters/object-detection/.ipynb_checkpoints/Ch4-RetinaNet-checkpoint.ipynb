{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mp_NYLoZMcIf"
   },
   "source": [
    "# 4. RetinaNet\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "osFFm5Qjc8gW"
   },
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Pseudo-Lab/Tutorial-Book/blob/master/book/chapters/object-detection/Ch4-RetinaNet.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 334
    },
    "executionInfo": {
     "elapsed": 775,
     "status": "ok",
     "timestamp": 1607650820645,
     "user": {
      "displayName": "안성진",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiCjgkN_MvtrSUHRuFvstrWm6fhi5cf7CKd2UHYAw=s64",
      "userId": "00266029492778998652"
     },
     "user_tz": -540
    },
    "id": "HbcoYV0uc7yr",
    "outputId": "a7a0774c-e12c-42e6-8d45-fde87668a057",
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/rkHc_Tzn810\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "HTML('<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/rkHc_Tzn810\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KK93nhnJfoyQ"
   },
   "source": [
    "3장에서는 제공된 데이터에 augmentation을 가하는 방법과 데이터셋 클래스를 만드는 방법을 확인했습니다. 이번 장에서는 torchvision에서 제공하는 one-stage 모델인 RetinaNet을 활용해 의료용 마스크 검출 모델을 구축해보겠습니다. \n",
    "\n",
    "4.1절부터 4.3절까지는 2장과 3장에서 확인한 내용을 바탕으로 데이터를 불러오고 훈련용, 시험용 데이터로 나눈 후 데이터셋 클래스를 정의하겠습니다. 4.4절에서는 torchvision API를 활용하여 사전 훈련된 모델을 불러오겠습니다. 4.5절에서는 전이 학습을 통해 모델 학습을 진행한 후 4.6절에서 예측값 산출 및 모델 성능을 확인해보겠습니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hagQJQhNH3tx"
   },
   "source": [
    "## 4.1 데이터 다운로드"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "84tdB7_Lfrne"
   },
   "source": [
    "모델링 실습을 위해 2.1절에 나온 코드를 활용하여 데이터를 불러오겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7627,
     "status": "ok",
     "timestamp": 1606505906985,
     "user": {
      "displayName": "안성진",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiCjgkN_MvtrSUHRuFvstrWm6fhi5cf7CKd2UHYAw=s64",
      "userId": "00266029492778998652"
     },
     "user_tz": -540
    },
    "id": "TWRFeIewiogs",
    "outputId": "05d238ae-e4a6-4146-c1dc-4fa919a7ff9b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'Tutorial-Book-Utils'...\n",
      "remote: Enumerating objects: 12, done.\u001b[K\n",
      "remote: Counting objects: 100% (12/12), done.\u001b[K\n",
      "remote: Compressing objects: 100% (11/11), done.\u001b[K\n",
      "remote: Total 12 (delta 1), reused 2 (delta 0), pack-reused 0\u001b[K\n",
      "Unpacking objects: 100% (12/12), done.\n",
      "Face Mask Detection.zip is done!\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/Pseudo-Lab/Tutorial-Book-Utils\n",
    "!python Tutorial-Book-Utils/PL_data_loader.py --data FaceMaskDetection\n",
    "!unzip -q Face\\ Mask\\ Detection.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JzK1cPwJJu_d"
   },
   "source": [
    "## 4.2 데이터 분리"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bTn5bLPVftSn"
   },
   "source": [
    "3.3절에서 확인한 데이터 분리 방법을 활용하여 데이터를 분리하겠습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 721,
     "status": "ok",
     "timestamp": 1606506027461,
     "user": {
      "displayName": "안성진",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiCjgkN_MvtrSUHRuFvstrWm6fhi5cf7CKd2UHYAw=s64",
      "userId": "00266029492778998652"
     },
     "user_tz": -540
    },
    "id": "PHqwbdjycpUP",
    "outputId": "f2481721-1cfa-4974-cdc1-b0d70309d5a7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "853\n",
      "853\n",
      "683\n",
      "683\n",
      "170\n",
      "170\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import shutil\n",
    "\n",
    "print(len(os.listdir('annotations')))\n",
    "print(len(os.listdir('images')))\n",
    "\n",
    "!mkdir test_images\n",
    "!mkdir test_annotations\n",
    "\n",
    "\n",
    "random.seed(1234)\n",
    "idx = random.sample(range(853), 170)\n",
    "\n",
    "for img in np.array(sorted(os.listdir('images')))[idx]:\n",
    "    shutil.move('images/'+img, 'test_images/'+img)\n",
    "\n",
    "for annot in np.array(sorted(os.listdir('annotations')))[idx]:\n",
    "    shutil.move('annotations/'+annot, 'test_annotations/'+annot)\n",
    "\n",
    "print(len(os.listdir('annotations')))\n",
    "print(len(os.listdir('images')))\n",
    "print(len(os.listdir('test_annotations')))\n",
    "print(len(os.listdir('test_images')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S3aR-WPJGbqr"
   },
   "source": [
    "## 4.3 데이터셋 클래스 정의\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AIFHVnmlfvSy"
   },
   "source": [
    "파이토치 모델을 학습시키기 위해선 데이터셋 클래스를 정의해야 합니다. torchvision에서 제공하는 객체 탐지 모델을 학습시키기 위한 데이터셋 클래스의 `__getitem__` 메서드는 이미지 파일과 바운딩 박스 좌표를 반환 합니다. 데이터셋 클래스를 3장에서 활용한 코드를 응용해 아래와 같이 정의 하겠습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6AW5nZyNio-2"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.patches as patches\n",
    "from bs4 import BeautifulSoup\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import numpy as np\n",
    "import time\n",
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "\n",
    "def generate_box(obj):\n",
    "    \n",
    "    xmin = float(obj.find('xmin').text)\n",
    "    ymin = float(obj.find('ymin').text)\n",
    "    xmax = float(obj.find('xmax').text)\n",
    "    ymax = float(obj.find('ymax').text)\n",
    "    \n",
    "    return [xmin, ymin, xmax, ymax]\n",
    "\n",
    "def generate_label(obj):\n",
    "\n",
    "    if obj.find('name').text == \"with_mask\":\n",
    "\n",
    "        return 1\n",
    "\n",
    "    elif obj.find('name').text == \"mask_weared_incorrect\":\n",
    "\n",
    "        return 2\n",
    "\n",
    "    return 0\n",
    "\n",
    "def generate_target(file): \n",
    "    with open(file) as f:\n",
    "        data = f.read()\n",
    "        soup = BeautifulSoup(data, \"html.parser\")\n",
    "        objects = soup.find_all(\"object\")\n",
    "\n",
    "        num_objs = len(objects)\n",
    "\n",
    "        boxes = []\n",
    "        labels = []\n",
    "        for i in objects:\n",
    "            boxes.append(generate_box(i))\n",
    "            labels.append(generate_label(i))\n",
    "\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32) \n",
    "        labels = torch.as_tensor(labels, dtype=torch.int64) \n",
    "        \n",
    "        target = {}\n",
    "        target[\"boxes\"] = boxes\n",
    "        target[\"labels\"] = labels\n",
    "        \n",
    "        return target\n",
    "\n",
    "def plot_image_from_output(img, annotation):\n",
    "    \n",
    "    img = img.cpu().permute(1,2,0)\n",
    "    \n",
    "    fig,ax = plt.subplots(1)\n",
    "    ax.imshow(img)\n",
    "    \n",
    "    for idx in range(len(annotation[\"boxes\"])):\n",
    "        xmin, ymin, xmax, ymax = annotation[\"boxes\"][idx]\n",
    "\n",
    "        if annotation['labels'][idx] == 0 :\n",
    "            rect = patches.Rectangle((xmin,ymin),(xmax-xmin),(ymax-ymin),linewidth=1,edgecolor='r',facecolor='none')\n",
    "        \n",
    "        elif annotation['labels'][idx] == 1 :\n",
    "            \n",
    "            rect = patches.Rectangle((xmin,ymin),(xmax-xmin),(ymax-ymin),linewidth=1,edgecolor='g',facecolor='none')\n",
    "            \n",
    "        else :\n",
    "        \n",
    "            rect = patches.Rectangle((xmin,ymin),(xmax-xmin),(ymax-ymin),linewidth=1,edgecolor='orange',facecolor='none')\n",
    "\n",
    "        ax.add_patch(rect)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "class MaskDataset(Dataset):\n",
    "    def __init__(self, path, transform=None):\n",
    "        self.path = path\n",
    "        self.imgs = list(sorted(os.listdir(self.path)))\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.imgs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_image = self.imgs[idx]\n",
    "        file_label = self.imgs[idx][:-3] + 'xml'\n",
    "        img_path = os.path.join(self.path, file_image)\n",
    "        \n",
    "        if 'test' in self.path:\n",
    "            label_path = os.path.join(\"test_annotations/\", file_label)\n",
    "        else:\n",
    "            label_path = os.path.join(\"annotations/\", file_label)\n",
    "\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        target = generate_target(label_path)\n",
    "        \n",
    "        to_tensor = torchvision.transforms.ToTensor()\n",
    "\n",
    "        if self.transform:\n",
    "            img, transform_target = self.transform(np.array(img), np.array(target['boxes']))\n",
    "            target['boxes'] = torch.as_tensor(transform_target)\n",
    "\n",
    "        # tensor로 변경\n",
    "        img = to_tensor(img)\n",
    "\n",
    "\n",
    "        return img, target\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "dataset = MaskDataset('images/')\n",
    "test_dataset = MaskDataset('test_images/')\n",
    "\n",
    "data_loader = torch.utils.data.DataLoader(dataset, batch_size=4, collate_fn=collate_fn)\n",
    "test_data_loader = torch.utils.data.DataLoader(test_dataset, batch_size=2, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e2kNQXFNL4Aw"
   },
   "source": [
    "최종적으로 훈련용 데이터와 시험용 데이터를 batch 단위로 불러올 수 있게 `torch.utils.data.DataLoader` 함수를 활용해 `data_loader`와 `test_data_loader`를 각각 정의합니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nRrFZq0AOcN3"
   },
   "source": [
    "## 4.4 모델 불러오기\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ux0kyJv5fyES"
   },
   "source": [
    "`torchvision`에서는 각종 컴퓨터 비전 문제를 해결하기 위한 딥러닝 모델을 쉽게 불러올 수 있는 API를 제공합니다. `torchvision.models` 모듈을 활용하여 RetinaNet 모델을 불러오도록 하겠습니다. RetinaNet은 `torchvision` 0.8.0 이상에서 제공되므로, 아래 코드를 활용하여 `torchvision` 버전을 맞춰줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4943,
     "status": "ok",
     "timestamp": 1606506425492,
     "user": {
      "displayName": "안성진",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiCjgkN_MvtrSUHRuFvstrWm6fhi5cf7CKd2UHYAw=s64",
      "userId": "00266029492778998652"
     },
     "user_tz": -540
    },
    "id": "Cr8H6JLey_1E",
    "outputId": "5254f8cb-2cc4-4b8e-9ac3-4af141cbab27"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
      "Requirement already satisfied: torch==1.7.0+cu101 in /usr/local/lib/python3.6/dist-packages (1.7.0+cu101)\n",
      "Requirement already satisfied: torchvision==0.8.1+cu101 in /usr/local/lib/python3.6/dist-packages (0.8.1+cu101)\n",
      "Collecting torchaudio==0.7.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3f/23/6b54106b3de029d3f10cf8debc302491c17630357449c900d6209665b302/torchaudio-0.7.0-cp36-cp36m-manylinux1_x86_64.whl (7.6MB)\n",
      "\u001b[K     |████████████████████████████████| 7.6MB 11.1MB/s \n",
      "\u001b[?25hRequirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch==1.7.0+cu101) (0.8)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch==1.7.0+cu101) (3.7.4.3)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch==1.7.0+cu101) (1.18.5)\n",
      "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch==1.7.0+cu101) (0.16.0)\n",
      "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision==0.8.1+cu101) (7.0.0)\n",
      "Installing collected packages: torchaudio\n",
      "Successfully installed torchaudio-0.7.0\n"
     ]
    }
   ],
   "source": [
    "!pip install torch==1.7.0+cu101 torchvision==0.8.1+cu101 torchaudio==0.7.0 -f https://download.pytorch.org/whl/torch_stable.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TvElOoznuOo_"
   },
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 585,
     "status": "ok",
     "timestamp": 1606509502433,
     "user": {
      "displayName": "안성진",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiCjgkN_MvtrSUHRuFvstrWm6fhi5cf7CKd2UHYAw=s64",
      "userId": "00266029492778998652"
     },
     "user_tz": -540
    },
    "id": "3JGtQrFSvZD6",
    "outputId": "b5872089-8430-4eb6-9a8a-4073ad562651"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'0.8.1+cu101'"
      ]
     },
     "execution_count": 4,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torchvision.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PCBtpaBmPaVL"
   },
   "source": [
    "`torchvision.__version__` 명령어를 통해 현재 cuda 10.1 버전에서 작동하는 `torchvision` 0.8.1 버전이 설치 됐음을 확인할 수 있습니다. 다음으로는 아래 코드를 실행하여 RetinaNet 모델을 불러옵니다. Face Mask Detection 데이터셋에 3개의 클래스가 존재하므로 num_classes 매개변수를 3으로 정의하고, 전이 학습을 할 것이기 때문에 backbone 구조는 사전 학습 된 가중치를, 그 외 가중치는 초기화 상태로 가져옵니다. backbone은 객체 탐지 데이터셋으로 유명한 Coco 데이터셋에 사전 학습 됐습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1EUUU5E_s_oV"
   },
   "outputs": [],
   "source": [
    "retina = torchvision.models.detection.retinanet_resnet50_fpn(num_classes = 3, pretrained=False, pretrained_backbone = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-3p2PLxpP8PQ"
   },
   "source": [
    "## 4.5 전이 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TpvFMGNff0Yn"
   },
   "source": [
    "모델을 불러왔으면 아래 코드를 활용하여 전이 학습을 진행합니다. (코드 셀 나눠서 설명 더 추가해야함)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7577122,
     "status": "ok",
     "timestamp": 1606292618162,
     "user": {
      "displayName": "안성진",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiCjgkN_MvtrSUHRuFvstrWm6fhi5cf7CKd2UHYAw=s64",
      "userId": "00266029492778998652"
     },
     "user_tz": -540
    },
    "id": "qBpMkUNVQQqo",
    "outputId": "c43c5c67-0bf6-401a-bf05-f9f5bfb08633"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/torch/nn/_reduction.py:44: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(285.9670, device='cuda:0', grad_fn=<AddBackward0>) time: 242.22558188438416\n",
      "tensor(268.1001, device='cuda:0', grad_fn=<AddBackward0>) time: 251.5482075214386\n",
      "tensor(248.4554, device='cuda:0', grad_fn=<AddBackward0>) time: 248.92862486839294\n",
      "tensor(233.0612, device='cuda:0', grad_fn=<AddBackward0>) time: 249.69438576698303\n",
      "tensor(234.2285, device='cuda:0', grad_fn=<AddBackward0>) time: 247.88670659065247\n",
      "tensor(202.4744, device='cuda:0', grad_fn=<AddBackward0>) time: 249.68517541885376\n",
      "tensor(172.9739, device='cuda:0', grad_fn=<AddBackward0>) time: 250.47061586380005\n",
      "tensor(125.8968, device='cuda:0', grad_fn=<AddBackward0>) time: 251.4771168231964\n",
      "tensor(102.0443, device='cuda:0', grad_fn=<AddBackward0>) time: 251.20848298072815\n",
      "tensor(88.1749, device='cuda:0', grad_fn=<AddBackward0>) time: 251.144877910614\n",
      "tensor(78.1594, device='cuda:0', grad_fn=<AddBackward0>) time: 251.8066761493683\n",
      "tensor(73.6921, device='cuda:0', grad_fn=<AddBackward0>) time: 251.669575214386\n",
      "tensor(69.6965, device='cuda:0', grad_fn=<AddBackward0>) time: 251.8230264186859\n",
      "tensor(63.9101, device='cuda:0', grad_fn=<AddBackward0>) time: 252.08272123336792\n",
      "tensor(56.2955, device='cuda:0', grad_fn=<AddBackward0>) time: 252.18470931053162\n",
      "tensor(56.2638, device='cuda:0', grad_fn=<AddBackward0>) time: 252.03237462043762\n",
      "tensor(50.2047, device='cuda:0', grad_fn=<AddBackward0>) time: 252.09569120407104\n",
      "tensor(45.9254, device='cuda:0', grad_fn=<AddBackward0>) time: 253.205641746521\n",
      "tensor(44.4599, device='cuda:0', grad_fn=<AddBackward0>) time: 253.05651235580444\n",
      "tensor(43.9277, device='cuda:0', grad_fn=<AddBackward0>) time: 253.1837260723114\n",
      "tensor(40.4117, device='cuda:0', grad_fn=<AddBackward0>) time: 253.18618297576904\n",
      "tensor(39.0882, device='cuda:0', grad_fn=<AddBackward0>) time: 253.36814761161804\n",
      "tensor(35.3732, device='cuda:0', grad_fn=<AddBackward0>) time: 253.41503262519836\n",
      "tensor(34.0460, device='cuda:0', grad_fn=<AddBackward0>) time: 252.93738174438477\n",
      "tensor(35.8844, device='cuda:0', grad_fn=<AddBackward0>) time: 253.25822925567627\n",
      "tensor(33.1177, device='cuda:0', grad_fn=<AddBackward0>) time: 253.25469851493835\n",
      "tensor(28.4753, device='cuda:0', grad_fn=<AddBackward0>) time: 253.2648823261261\n",
      "tensor(30.3831, device='cuda:0', grad_fn=<AddBackward0>) time: 253.4244725704193\n",
      "tensor(28.0954, device='cuda:0', grad_fn=<AddBackward0>) time: 253.57142424583435\n",
      "tensor(28.5899, device='cuda:0', grad_fn=<AddBackward0>) time: 253.16517424583435\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "num_epochs = 30\n",
    "retina.to(device)\n",
    "    \n",
    "# parameters\n",
    "params = [p for p in retina.parameters() if p.requires_grad] # gradient calculation이 필요한 params만 추출\n",
    "optimizer = torch.optim.SGD(params, lr=0.005,\n",
    "                                momentum=0.9, weight_decay=0.0005)\n",
    "\n",
    "len_dataloader = len(data_loader)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    start = time.time()\n",
    "    retina.train()\n",
    "\n",
    "    i = 0    \n",
    "    epoch_loss = 0\n",
    "    for images, targets in data_loader:\n",
    "        images = list(image.to(device) for image in images)\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        loss_dict = retina(images, targets) \n",
    "\n",
    "        losses = sum(loss for loss in loss_dict.values()) \n",
    "\n",
    "        i += 1\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += losses \n",
    "    print(epoch_loss, f'time: {time.time() - start}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uWdd-bPpRje0"
   },
   "source": [
    "모델 재사용을 위해 아래 코드를 실행하여 학습된 가중치를 저장해줍니다. `torch.save` 함수를 활용해 지정한 위치에 학습된 가중치를 저장할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "buEMhbtZRhSx"
   },
   "outputs": [],
   "source": [
    "torch.save(retina.state_dict(),f'retina_{num_epochs}.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5341,
     "status": "ok",
     "timestamp": 1606509535460,
     "user": {
      "displayName": "안성진",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiCjgkN_MvtrSUHRuFvstrWm6fhi5cf7CKd2UHYAw=s64",
      "userId": "00266029492778998652"
     },
     "user_tz": -540
    },
    "id": "YwOeuUCqGEc0",
    "outputId": "7e8b02c1-9b32-4ba2-c173-3ce61753974a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 6,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retina.load_state_dict(torch.load(f'retina_{num_epochs}.pt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c-6mROA6boUr"
   },
   "source": [
    "학습된 가중치를 불러올 때는 `load_state_dict`과 `torch.load`함수를 사용하면 됩니다. 만약 retina 변수를 새롭게 지정했을 경우, 해당 모델을 GPU 메모리에 올려주어야 GPU 연산이 가능합니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "apqJwlQGbi1E"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "retina.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5Yt8iHl7QHJj"
   },
   "source": [
    "## 4.6 예측"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7Wd0wEgvozWK"
   },
   "source": [
    "훈련이 마무리 되었으면, 예측 결과를 확인하도록 하겠습니다. test_data_loader에서 데이터를 불러와 모델에 넣어 학습 후, 예측된 결과와 실제 값을 각각 시각화 해보도록 하겠습니다. 우선 예측에 필요한 함수를 정의하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QNnJ31gVzch8"
   },
   "outputs": [],
   "source": [
    "def make_prediction(model, img, threshold):\n",
    "    model.eval()\n",
    "    preds = model(img)\n",
    "    for id in range(len(preds)) :\n",
    "        idx_list = []\n",
    "\n",
    "        for idx, score in enumerate(preds[id]['scores']) :\n",
    "            if score > threshold : #threshold 넘는 idx 구함\n",
    "                idx_list.append(idx)\n",
    "\n",
    "        preds[id]['boxes'] = preds[id]['boxes'][idx_list]\n",
    "        preds[id]['labels'] = preds[id]['labels'][idx_list]\n",
    "        preds[id]['scores'] = preds[id]['scores'][idx_list]\n",
    "\n",
    "\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ib8vthZOX5nt"
   },
   "source": [
    "`make_prediction` 함수에는 학습된 딥러닝 모델을 활용해 예측하는 알고리즘이 저장돼 있습니다. `threshold` 파라미터를 조정해 신뢰도가 일정 수준 이상의 바운딩 박스만 선택합니다. 보통 0.5 이상인 값을 최종 선택합니다. 다음으로는 for문을 활용해 `test_data_loader`에 있는 모든 데이터에 대해 예측을 실시하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 25283,
     "status": "ok",
     "timestamp": 1606509709274,
     "user": {
      "displayName": "안성진",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiCjgkN_MvtrSUHRuFvstrWm6fhi5cf7CKd2UHYAw=s64",
      "userId": "00266029492778998652"
     },
     "user_tz": -540
    },
    "id": "Xhb-l1gPSbXA",
    "outputId": "b7542333-51d7-4eef-9879-b12135c6d8c8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 85/85 [00:24<00:00,  3.47it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "labels = []\n",
    "preds_adj_all = []\n",
    "annot_all = []\n",
    "\n",
    "for im, annot in tqdm(test_data_loader, position = 0, leave = True):\n",
    "    im = list(img.to(device) for img in im)\n",
    "    #annot = [{k: v.to(device) for k, v in t.items()} for t in annot]\n",
    "\n",
    "    for t in annot:\n",
    "        labels += t['labels']\n",
    "\n",
    "    with torch.no_grad():\n",
    "        preds_adj = make_prediction(retina, im, 0.5)\n",
    "        preds_adj = [{k: v.to(torch.device('cpu')) for k, v in t.items()} for t in preds_adj]\n",
    "        preds_adj_all.append(preds_adj)\n",
    "        annot_all.append(annot)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5WmaVxDmYn5Y"
   },
   "source": [
    "`tqdm` 함수를 활용해 진행 상황을 표츌하고 있습니다. 예측된 모든 값은 `preds_adj_all` 변수에 저장됐습니다. 다음으로는 실제 바운딩 박스와 예측한 바운딩 박스에 대한 시각화를 진행해보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "output_embedded_package_id": "1rIbtuvTZx9YlR1RavfZLh0vXyZgGwV65"
    },
    "executionInfo": {
     "elapsed": 5151,
     "status": "ok",
     "timestamp": 1606509717388,
     "user": {
      "displayName": "안성진",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiCjgkN_MvtrSUHRuFvstrWm6fhi5cf7CKd2UHYAw=s64",
      "userId": "00266029492778998652"
     },
     "user_tz": -540
    },
    "id": "0lFxRwSdSBwj",
    "outputId": "f213a1c8-48d0-4bac-dd2e-a0034e33a148"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Output hidden; open in https://colab.research.google.com to view."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "batch_i = 0\n",
    "for im, annot in test_data_loader:\n",
    "    for sample_i in range(len(im)) :\n",
    "        print('true')\n",
    "        plot_image_from_output(im[sample_i], annot[sample_i])\n",
    "        print('pred')\n",
    "        plot_image_from_output(im[sample_i], preds_adj_all[batch_i][sample_i])\n",
    "    batch_i += 1\n",
    "    if batch_i == 4:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NTvuyHGXZBBH"
   },
   "source": [
    "for문을 활용해 4개의 batch, 총 8개의 이미지에 대한 실제 값과 예측 값을 시각해 보았습니다. 마스크 착용자는 잘 탐지하고 있는 것을 관측하고 있으며, 마스크 미착용자에 대해서는 가끔씩 마스크를 올바르게 착용하지 않는 것으로 탐지하는 것을 볼 수 있습니다. 전반적인 모델 성능을 평가하기 위해 mean Average Precision (mAP)를 산출해보겠습니다. mAP는 객체 탐지 모델을 평가할 때 사용하는 지표입니다. (추가 설명 입력)\n",
    "\n",
    "데이터 다운로드시 불러왔던 Tutorial-Book-Utils 폴더 내에는 utils_ObjectDetection.py 파일이 있습니다. 해당 모듈 내에 있는 함수를 활용해 mAP를 산출해보겠습니다. 우선 utils_ObjectDetection.py 모듈을 불러옵니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 712,
     "status": "ok",
     "timestamp": 1606509731197,
     "user": {
      "displayName": "안성진",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiCjgkN_MvtrSUHRuFvstrWm6fhi5cf7CKd2UHYAw=s64",
      "userId": "00266029492778998652"
     },
     "user_tz": -540
    },
    "id": "hGOG2JGPZ2Qg",
    "outputId": "56f2e93e-bd9c-4de6-da3a-a72745045795"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/Tutorial-Book-Utils\n"
     ]
    }
   ],
   "source": [
    "%cd Tutorial-Book-Utils/\n",
    "import utils_ObjectDetection as utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nQLausRKTB16"
   },
   "outputs": [],
   "source": [
    "sample_metrics = []\n",
    "for batch_i in range(len(preds_adj_all)):\n",
    "    sample_metrics += utils.get_batch_statistics(preds_adj_all[batch_i], annot_all[batch_i], iou_threshold=0.5) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YP56-gyddy9P"
   },
   "source": [
    "batch 별 mAP를 산출하는데 필요한 정보를 `sample_metrics`에 저장 후 `ap_per_class`함수를 활용해 mAP를 산출합니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 759,
     "status": "ok",
     "timestamp": 1606509884671,
     "user": {
      "displayName": "안성진",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiCjgkN_MvtrSUHRuFvstrWm6fhi5cf7CKd2UHYAw=s64",
      "userId": "00266029492778998652"
     },
     "user_tz": -540
    },
    "id": "uxsYNiXacYxQ",
    "outputId": "11f2317b-3269-4b31-b901-fab685c5ba58"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mAP : 0.5824690281035101\n",
      "AP : tensor([0.7684, 0.9188, 0.0603], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "true_positives, pred_scores, pred_labels = [torch.cat(x, 0) for x in list(zip(*sample_metrics))]  # 배치가 전부 합쳐짐\n",
    "precision, recall, AP, f1, ap_class = utils.ap_per_class(true_positives, pred_scores, pred_labels, torch.tensor(labels))\n",
    "mAP = torch.mean(AP)\n",
    "print(f'mAP : {mAP}')\n",
    "print(f'AP : {AP}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "njTGWnKGd8l-"
   },
   "source": [
    "결과를 해석하면 0번 클래스인 마스크를 미착용한 객체에 대해서는 0.7684 AP를 보이며 1번 클래스인 마스크 착용 객체에 대해서는 0.9188 AP를 보이고, 2번 클래스인 마스크를 올바르게 착용하지 않은 객체에 대해서는 0.06 AP를 보입니다. (결과 해석 내용 보완하기)\n",
    "\n",
    "지금까지 RetinaNet에 대한 전이 학습을 실시해 의료용 마스크 탐지 모델을 만들어 보았습니다. 다음 장에서는 Two-Stage Detector인 Faster R-CNN을 활용해 탐지 성능을 높여보겠습니다. "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Ch4 데이터 모델링.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
