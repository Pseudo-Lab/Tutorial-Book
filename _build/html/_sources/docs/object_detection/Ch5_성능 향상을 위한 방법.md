# 5. ì„±ëŠ¥ í–¥ìƒì„ ìœ„í•œ ë°©ë²•

ì´ë²ˆ ì¥ì—ì„œëŠ” 2-stage-methodì¸ Faster R-CNNìœ¼ë¡œ ê°ì²´ íƒì§€ë¥¼ í•´ë³´ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤.

ì‹¤í—˜ì— ì•ì„œ Google Colabì—ì„œëŠ” ëœë¤ GPUë¥¼ í• ë‹¹í•˜ê³  ìˆê¸° ë•Œë¬¸ì— ë©”ëª¨ë¦¬ ë¶€ì¡±í˜„ìƒì´ ì¼ì–´ë‚  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ë¨¼ì € GPUë¥¼ í™•ì¸ í›„ì— ë©”ëª¨ë¦¬ê°€ ì¶©ë¶„í•  ê²½ìš° ì‹¤í—˜ì„ í•˜ì‹œê¸¸ ê¶Œì¥í•©ë‹ˆë‹¤.
(ëŸ°íƒ€ì„ì„ ì´ˆê¸°í™”í•  ê²½ìš° ìƒˆë¡œìš´ GPUë¥¼ í• ë‹¹ë°›ìœ¼ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤)


```python
import torch

if torch.cuda.is_available():    
    device = torch.device("cuda")
    print('There are %d GPU(s) available.' % torch.cuda.device_count())
    print('We will use the GPU:', torch.cuda.get_device_name(0))

else:
    print('No GPU available, using the CPU instead.')
    device = torch.device("cpu")
```

    There are 1 GPU(s) available.
    We will use the GPU: Tesla T4
    

##5.1 ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°
2.2ì ˆì—ì„œ ë³´ì•˜ë“¯ì´ ê°€ì§œì—°êµ¬ì†Œ APIë¥¼ í™œìš©í•˜ì—¬ ë°ì´í„°ë¥¼ ë‹¤ìš´ë°›ê³  ì••ì¶• íŒŒì¼ì„ í’€ì–´ì¤ë‹ˆë‹¤.



```python
!git clone https://github.com/Pseudo-Lab/Tutorial-Book-DataLoader
!python Tutorial-Book-DataLoader/PL_data_loader.py --data FaceMaskDetection
!unzip -q Face\ Mask\ Detection.zip
```

    Cloning into 'Tutorial-Book-DataLoader'...
    remote: Enumerating objects: 9, done.[K
    remote: Counting objects: 100% (9/9), done.[K
    remote: Compressing objects: 100% (8/8), done.[K
    remote: Total 9 (delta 1), reused 2 (delta 0), pack-reused 0[K
    Unpacking objects: 100% (9/9), done.
    Face Mask Detection.zip is done!
    

## 5.2 ë°ì´í„° ë¶„ë¦¬
ë¨¼ì € ì´ì „ì¥ì—ì„œ ì‚¬ìš©í–ˆë˜ íŒ¨í‚¤ì§€ë“¤ê³¼ ëª¨ë¸ì— í•„ìš”í•œ íŒ¨í‚¤ì§€ë¥¼ ë¶ˆëŸ¬ì˜¤ê² ìŠµë‹ˆë‹¤.

`torchvision`ì€ ì´ë¯¸ì§€ ì²˜ë¦¬ë¥¼ í•˜ê¸° ìœ„í•´ ì‚¬ìš©ë˜ë©° ë°ì´í„°ì…‹ì— ê´€í•œ íŒ¨í‚¤ì§€ì™€ ëª¨ë¸ì— ê´€í•œ íŒ¨í‚¤ì§€ê°€ ë‚´ì¥ë˜ì–´ ìˆìŠµë‹ˆë‹¤.



```python
import os
import numpy as np
import matplotlib.patches as patches
import matplotlib.pyplot as plt
from bs4 import BeautifulSoup
from PIL import Image
import torchvision
from torchvision import transforms, datasets, models
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
import time
```

ê·¸ë¦¬ê³  3.4ì ˆê³¼ ê°™ì´ ì´ 853ì¥ì˜ imagesì™€ annotationsì„ í•™ìŠµ ë°ì´í„°ì™€ ì‹œí—˜ ë°ì´í„°ë¡œ ë¶„ë¦¬í•©ë‹ˆë‹¤.

test í´ë”ë¥¼ ë§Œë“¤ì–´ 8:2ì˜ ë¹„ìœ¨ë¡œ ì‹œí—˜ ë°ì´í„°ë¥¼ ë”°ë¡œ ì˜®ê²¨ì¤ë‹ˆë‹¤.


```python
print(len(os.listdir('annotations')))
print(len(os.listdir('images')))
```

    853
    853
    


```python
!mkdir test_images
!mkdir test_annotations
```


```python
import shutil

for img in sorted(os.listdir('images'))[-170:]:
    shutil.move('images/'+img, 'test_images/'+img)

for annot in sorted(os.listdir('annotations'))[-170:]:
    shutil.move('annotations/'+annot, 'test_annotations/'+annot)
```

ìœ„ì˜ ì½”ë“œì—ì„œ shutil íŒ¨í‚¤ì§€ëŠ” íŒŒì¼ê³¼ í´ë” ê´€ë ¨í•œ ì´ë™, ë³µì‚¬ ì‘ì—…ì„ í•  ë•Œ ì‚¬ìš©í•©ë‹ˆë‹¤.


```python
print(len(os.listdir('annotations')))
print(len(os.listdir('images')))
print(len(os.listdir('test_annotations')))
print(len(os.listdir('test_images')))
```

    683
    683
    170
    170
    

## 5.3 ë°ì´í„°ì…‹ í´ë˜ìŠ¤ ì •ì˜
2.3ì ˆê³¼ ë§ˆì°¬ê°€ì§€ë¡œ ë°”ìš´ë”© ë°•ìŠ¤ë¥¼ ìœ„í•œ í•¨ìˆ˜ë“¤ì„ ì •ì˜í•´ì¤ë‹ˆë‹¤.


```python
def generate_box(obj):
    
    xmin = int(obj.find('xmin').text)
    ymin = int(obj.find('ymin').text)
    xmax = int(obj.find('xmax').text)
    ymax = int(obj.find('ymax').text)
    
    return [xmin, ymin, xmax, ymax]

def generate_label(obj):
    if obj.find('name').text == "with_mask":
        return 1
    elif obj.find('name').text == "mask_weared_incorrect":
        return 2
    return 0

def generate_target(image_id, file): 
    with open(file) as f:
        data = f.read()
        soup = BeautifulSoup(data, 'html.parser')
        objects = soup.find_all('object')

        num_objs = len(objects)

        # Bounding boxes for objects
        # In coco format, bbox = [xmin, ymin, width, height]
        # In pytorch, the input should be [xmin, ymin, xmax, ymax]
        boxes = []
        labels = []
        for i in objects:
            boxes.append(generate_box(i))
            labels.append(generate_label(i))
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        # Labels (In my case, I only one class: target class or background)
        labels = torch.as_tensor(labels, dtype=torch.int64)
        # Tensorise img_id
        img_id = torch.tensor([image_id])
        # Annotation is in dictionary format
        target = {}
        target["boxes"] = boxes
        target["labels"] = labels
        target["image_id"] = img_id
        
        return target
```

ë˜í•œ 4.3ì ˆì²˜ëŸ¼ ë°ì´í„°ì…‹ í´ë˜ìŠ¤ì™€ ë°ì´í„° ë¡œë”ë¥¼ ì •ì˜í•´ì¤ë‹ˆë‹¤.

ë°ì´í„°ì…‹ì€  `torch.utils.data.DataLoader` í•¨ìˆ˜ë¥¼ í†µí•´ ë°°ì¹˜ ì‚¬ì´ì¦ˆëŠ” 4ë¡œ ì§€ì •í•˜ì—¬ ë¶ˆëŸ¬ì˜¤ê² ìŠµë‹ˆë‹¤.

ë°°ì¹˜ ì‚¬ì´ì¦ˆëŠ” ê°œì¸ì˜ ë©”ëª¨ë¦¬ í¬ê¸°ì— ë”°ë¼ ììœ ë¡­ê²Œ ì„¤ì •í•˜ë©´ ë©ë‹ˆë‹¤. 


```python
class MaskDataset(object):
    def __init__(self, transforms, path):
        '''
        path: path to train folder or test folder
        '''
        # transform moduleê³¼ img path ê²½ë¡œë¥¼ ì •ì˜
        self.transforms = transforms
        # load all image files, sorting them to
        # ensure that they are aligned
        self.path = path
        self.imgs = list(sorted(os.listdir(self.path)))


    def __getitem__(self, idx): #special method
        # load images ad masks
        file_image = self.imgs[idx]
        file_label = self.imgs[idx][:-3] + 'xml'
        img_path = os.path.join(self.path, file_image)
        
        if 'test' in self.path:
            label_path = os.path.join("test_annotations/", file_label)
        else:
            label_path = os.path.join("annotations/", file_label)

        img = Image.open(img_path).convert("RGB")
        #Generate Label
        target = generate_target(idx, label_path)
        
        if self.transforms is not None:
            img = self.transforms(img)

        return img, target

    def __len__(self): # len() ì ìš©ê°€ëŠ¥ì¼€ í•¨, special method
        return len(self.imgs)

data_transform = transforms.Compose([  # transforms.Compose : list ë‚´ì˜ ì‘ì—…ì„ ì—°ë‹¬ì•„ í•  ìˆ˜ ìˆê²Œ í˜¸ì¶œí•˜ëŠ” í´ë˜ìŠ¤
        transforms.ToTensor() # ToTensor : numpy ì´ë¯¸ì§€ì—ì„œ torch ì´ë¯¸ì§€ë¡œ ë³€ê²½
    ])

def collate_fn(batch):
    return tuple(zip(*batch))

dataset = MaskDataset(data_transform, 'images/')
test_dataset = MaskDataset(data_transform, 'test_images/')

data_loader = torch.utils.data.DataLoader(dataset, batch_size=4, collate_fn=collate_fn)
test_data_loader = torch.utils.data.DataLoader(test_dataset, batch_size=4, collate_fn=collate_fn)
```

##5.4 ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°
`torchvision.models.detection`ì—ì„œëŠ” Faster R-CNN API(`torchvision.models.detection.fasterrcnn_resnet50_fpn`)ë¥¼ ì œê³µí•˜ê³  ìˆì–´ ì‰½ê²Œ êµ¬í˜„ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤.

ì´ëŠ” COCO ë°ì´í„°ì…‹ì„ ResNet50ìœ¼ë¡œ pre-trainedí•œ ëª¨ë¸ì„ ì œê³µí•˜ê³  ìˆìœ¼ë©°, `pretrained=True/False`ë¡œ ì„¤ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ê·¸ë¦¬ê³  ì´í›„ ëª¨ë¸ì„ ë¶ˆëŸ¬ì˜¬ ë•Œ `num_classes`ì— ì›í•˜ëŠ” í´ë˜ìŠ¤ ê°œìˆ˜ë¥¼ ì„¤ì •í•˜ê³  ëª¨ë¸ì„ ì‚¬ìš©í•˜ë©´ ë©ë‹ˆë‹¤.

ì´ë ‡ê²Œ ê°„ë‹¨í•˜ê²Œ Faster R-CNN ëª¨ë¸ì„ ì‹¤í—˜í•´ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.


```python
def get_model_instance_segmentation(num_classes):
  
    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)
    in_features = model.roi_heads.box_predictor.cls_score.in_features
    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)

    return model
```

##5.5 ì „ì´í•™ìŠµ
ê·¸ëŸ¼ Face Mask Detectionì— ì ìš©í•´ë³´ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤.

Face Mask Detection ë°ì´í„°ì…‹ì€ 3ê°œì˜ í´ë˜ìŠ¤ë¡œ ì´ë£¨ì–´ì ¸ ìˆê¸° ë•Œë¬¸ì— `num_classes`ë¥¼ 3ìœ¼ë¡œ ì„¤ì •í•œ í›„ ëª¨ë¸ì„ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.

GPUë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” í™˜ê²½ì´ë¼ë©´ deviceë¡œ ì§€ì •í•˜ì—¬ ë¶ˆëŸ¬ì˜¨ ëª¨ë¸ì„ GPUì— ë³´ë‚´ì¤ë‹ˆë‹¤.


```python
model = get_model_instance_segmentation(3)

device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu') 
model.to(device)
```

    Downloading: "https://download.pytorch.org/models/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth" to /root/.cache/torch/hub/checkpoints/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth
    


    HBox(children=(FloatProgress(value=0.0, max=167502836.0), HTML(value='')))


    
    




    FasterRCNN(
      (transform): GeneralizedRCNNTransform(
          Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
          Resize(min_size=(800,), max_size=1333, mode='bilinear')
      )
      (backbone): BackboneWithFPN(
        (body): IntermediateLayerGetter(
          (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
          (bn1): FrozenBatchNorm2d(64)
          (relu): ReLU(inplace=True)
          (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
          (layer1): Sequential(
            (0): Bottleneck(
              (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn1): FrozenBatchNorm2d(64)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): FrozenBatchNorm2d(64)
              (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn3): FrozenBatchNorm2d(256)
              (relu): ReLU(inplace=True)
              (downsample): Sequential(
                (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (1): FrozenBatchNorm2d(256)
              )
            )
            (1): Bottleneck(
              (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn1): FrozenBatchNorm2d(64)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): FrozenBatchNorm2d(64)
              (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn3): FrozenBatchNorm2d(256)
              (relu): ReLU(inplace=True)
            )
            (2): Bottleneck(
              (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn1): FrozenBatchNorm2d(64)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): FrozenBatchNorm2d(64)
              (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn3): FrozenBatchNorm2d(256)
              (relu): ReLU(inplace=True)
            )
          )
          (layer2): Sequential(
            (0): Bottleneck(
              (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn1): FrozenBatchNorm2d(128)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
              (bn2): FrozenBatchNorm2d(128)
              (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn3): FrozenBatchNorm2d(512)
              (relu): ReLU(inplace=True)
              (downsample): Sequential(
                (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
                (1): FrozenBatchNorm2d(512)
              )
            )
            (1): Bottleneck(
              (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn1): FrozenBatchNorm2d(128)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): FrozenBatchNorm2d(128)
              (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn3): FrozenBatchNorm2d(512)
              (relu): ReLU(inplace=True)
            )
            (2): Bottleneck(
              (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn1): FrozenBatchNorm2d(128)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): FrozenBatchNorm2d(128)
              (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn3): FrozenBatchNorm2d(512)
              (relu): ReLU(inplace=True)
            )
            (3): Bottleneck(
              (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn1): FrozenBatchNorm2d(128)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): FrozenBatchNorm2d(128)
              (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn3): FrozenBatchNorm2d(512)
              (relu): ReLU(inplace=True)
            )
          )
          (layer3): Sequential(
            (0): Bottleneck(
              (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn1): FrozenBatchNorm2d(256)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
              (bn2): FrozenBatchNorm2d(256)
              (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn3): FrozenBatchNorm2d(1024)
              (relu): ReLU(inplace=True)
              (downsample): Sequential(
                (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
                (1): FrozenBatchNorm2d(1024)
              )
            )
            (1): Bottleneck(
              (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn1): FrozenBatchNorm2d(256)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): FrozenBatchNorm2d(256)
              (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn3): FrozenBatchNorm2d(1024)
              (relu): ReLU(inplace=True)
            )
            (2): Bottleneck(
              (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn1): FrozenBatchNorm2d(256)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): FrozenBatchNorm2d(256)
              (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn3): FrozenBatchNorm2d(1024)
              (relu): ReLU(inplace=True)
            )
            (3): Bottleneck(
              (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn1): FrozenBatchNorm2d(256)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): FrozenBatchNorm2d(256)
              (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn3): FrozenBatchNorm2d(1024)
              (relu): ReLU(inplace=True)
            )
            (4): Bottleneck(
              (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn1): FrozenBatchNorm2d(256)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): FrozenBatchNorm2d(256)
              (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn3): FrozenBatchNorm2d(1024)
              (relu): ReLU(inplace=True)
            )
            (5): Bottleneck(
              (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn1): FrozenBatchNorm2d(256)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): FrozenBatchNorm2d(256)
              (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn3): FrozenBatchNorm2d(1024)
              (relu): ReLU(inplace=True)
            )
          )
          (layer4): Sequential(
            (0): Bottleneck(
              (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn1): FrozenBatchNorm2d(512)
              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
              (bn2): FrozenBatchNorm2d(512)
              (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn3): FrozenBatchNorm2d(2048)
              (relu): ReLU(inplace=True)
              (downsample): Sequential(
                (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
                (1): FrozenBatchNorm2d(2048)
              )
            )
            (1): Bottleneck(
              (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn1): FrozenBatchNorm2d(512)
              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): FrozenBatchNorm2d(512)
              (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn3): FrozenBatchNorm2d(2048)
              (relu): ReLU(inplace=True)
            )
            (2): Bottleneck(
              (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn1): FrozenBatchNorm2d(512)
              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): FrozenBatchNorm2d(512)
              (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn3): FrozenBatchNorm2d(2048)
              (relu): ReLU(inplace=True)
            )
          )
        )
        (fpn): FeaturePyramidNetwork(
          (inner_blocks): ModuleList(
            (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
            (1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
            (2): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
            (3): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
          )
          (layer_blocks): ModuleList(
            (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          )
          (extra_blocks): LastLevelMaxPool()
        )
      )
      (rpn): RegionProposalNetwork(
        (anchor_generator): AnchorGenerator()
        (head): RPNHead(
          (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (cls_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))
          (bbox_pred): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))
        )
      )
      (roi_heads): RoIHeads(
        (box_roi_pool): MultiScaleRoIAlign()
        (box_head): TwoMLPHead(
          (fc6): Linear(in_features=12544, out_features=1024, bias=True)
          (fc7): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (box_predictor): FastRCNNPredictor(
          (cls_score): Linear(in_features=1024, out_features=3, bias=True)
          (bbox_pred): Linear(in_features=1024, out_features=12, bias=True)
        )
      )
    )



ìœ„ì˜ ì¶œë ¥ë˜ëŠ” ê²°ê³¼ë¥¼ í†µí•´ Fastser R-CNNì´ ì–´ë–¤ layerë“¤ë¡œ êµ¬ì„±ë˜ì–´ ìˆëŠ”ì§€ ì•Œ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ì´ ë•Œ, GPU ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ëŠ” `torch.cuda.is_available()`ë¥¼ í†µí•´ ì•Œ ìˆ˜ ìˆìŠµë‹ˆë‹¤.


```python
torch.cuda.is_available()
```




    True



ì´ì œ ëª¨ë¸ì´ ë§Œë“¤ì–´ì¡Œìœ¼ë‹ˆ í•™ìŠµì„ í•´ë³´ê² ìŠµë‹ˆë‹¤.

í•™ìŠµ íšŸìˆ˜(`num_epochs`)ëŠ” 30ìœ¼ë¡œ ì§€ì •í•˜ê³ , SGD ë°©ë²•ì„ í†µí•´ ìµœì í™” ì‹œì¼œë³´ê² ìŠµë‹ˆë‹¤.

ê° í•˜ì´í¼ íŒŒë¼ë¯¸í„°ëŠ” ììœ ë¡­ê²Œ ìˆ˜ì •í•˜ì—¬ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.


```python
num_epochs = 30
params = [p for p in model.parameters() if p.requires_grad]
optimizer = torch.optim.SGD(params, lr=0.005,
                                momentum=0.9, weight_decay=0.0005)
```

ì´ì œ í•™ìŠµì„ ì‹œì¼œë³´ê² ìŠµë‹ˆë‹¤.

ìœ„ì—ì„œ ìƒì„±í•œ data_loaderì—ì„œ í•œ ë°°ì¹˜ì”© ìˆœì„œëŒ€ë¡œ ëª¨ë¸ì— ì‚¬ìš©í•˜ë©°, ì´í›„ loss ê³„ì‚°ì„ í†µí•´ ìµœì í™”ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.

ì§€ì •í•œ ì—í­ë§Œí¼ ë°˜ë³µí•˜ê³  ê° ì—í­ë§ˆë‹¤ lossë¥¼ ì¶œë ¥í•˜ì—¬ í™•ì¸í•©ë‹ˆë‹¤.


```python
print('----------------------train start--------------------------')
for epoch in range(num_epochs):
    start = time.time()
    model.train()
    i = 0    
    epoch_loss = 0
    for imgs, annotations in data_loader:
        i += 1
        imgs = list(img.to(device) for img in imgs)
        annotations = [{k: v.to(device) for k, v in t.items()} for t in annotations]
        loss_dict = model(imgs, annotations) 
        losses = sum(loss for loss in loss_dict.values())        

        optimizer.zero_grad()
        losses.backward()
        optimizer.step() 
        epoch_loss += losses
    print(f'epoch : {epoch+1}, Loss : {epoch_loss}, time : {time.time() - start}')
```

    ----------------------train start--------------------------
    epoch : 1, Loss : 57.07004928588867, time : 269.2427554130554
    epoch : 2, Loss : 39.43512725830078, time : 268.7257990837097
    epoch : 3, Loss : 32.92606735229492, time : 267.9897265434265
    epoch : 4, Loss : 28.937482833862305, time : 268.75047993659973
    epoch : 5, Loss : 28.95316505432129, time : 269.1901903152466
    epoch : 6, Loss : 26.549741744995117, time : 268.8914408683777
    epoch : 7, Loss : 23.126197814941406, time : 269.02998542785645
    epoch : 8, Loss : 24.639511108398438, time : 269.36951088905334
    epoch : 9, Loss : 22.880359649658203, time : 268.1531283855438
    epoch : 10, Loss : 19.464181900024414, time : 269.66372299194336
    epoch : 11, Loss : 19.465904235839844, time : 269.33947587013245
    epoch : 12, Loss : 19.406227111816406, time : 269.8699383735657
    epoch : 13, Loss : 17.55661964416504, time : 269.1940002441406
    epoch : 14, Loss : 18.008996963500977, time : 269.1736307144165
    epoch : 15, Loss : 17.95616912841797, time : 268.4821572303772
    epoch : 16, Loss : 15.526053428649902, time : 269.18503165245056
    epoch : 17, Loss : 14.976873397827148, time : 269.7834367752075
    epoch : 18, Loss : 16.75909996032715, time : 269.47766184806824
    epoch : 19, Loss : 16.092985153198242, time : 266.8096544742584
    epoch : 20, Loss : 15.812982559204102, time : 266.26549243927
    epoch : 21, Loss : 15.693418502807617, time : 266.25178718566895
    epoch : 22, Loss : 16.75595474243164, time : 266.33044385910034
    epoch : 23, Loss : 17.68772315979004, time : 266.0242862701416
    epoch : 24, Loss : 16.830270767211914, time : 265.3167517185211
    epoch : 25, Loss : 15.386274337768555, time : 264.8526291847229
    epoch : 26, Loss : 15.757489204406738, time : 265.6499161720276
    epoch : 27, Loss : 16.870962142944336, time : 265.70189142227173
    epoch : 28, Loss : 13.835110664367676, time : 264.8233985900879
    epoch : 29, Loss : 12.407930374145508, time : 264.04496908187866
    epoch : 30, Loss : 12.587990760803223, time : 264.01971983909607
    

í•™ìŠµì‹œí‚¨ ê°€ì¤‘ì¹˜ë¥¼ ì €ì¥í•˜ê³  ì‹¶ë‹¤ë©´, `torch.save`ë¥¼ ì´ìš©í•˜ì—¬ ì €ì¥í•´ë‘ê³  ë‚˜ì¤‘ì— ì–¸ì œë“ ì§€ ë¶ˆëŸ¬ì™€ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.


```python
torch.save(model.state_dict(),f'model_{num_epochs}.pt')
```


```python
model.load_state_dict(torch.load(f'model_{num_epochs}.pt'))
```




    <All keys matched successfully>



##5.6 ì˜ˆì¸¡
ëª¨ë¸ í•™ìŠµì´ ëë‚¬ìœ¼ë©´ ì˜ í•™ìŠµë˜ì—ˆëŠ”ì§€ ì˜ˆì¸¡ ê²°ê³¼ë¥¼ í™•ì¸í•´ë³´ê² ìŠµë‹ˆë‹¤.

ìš°ì„  test_data_loaderì˜ ì²«ë²ˆì§¸ ë°°ì¹˜ì— ëŒ€í•´ì„œ ê²°ê³¼ë¥¼ ì¶œë ¥í•´ë³´ì•˜ìŠµë‹ˆë‹¤.

ì•„ë˜ì™€ ê°™ì´ 4ì¥ì˜ ì´ë¯¸ì§€(1ë°°ì¹˜)ì— ëŒ€í•´ì„œ ë°”ìš´ë”© ë°•ìŠ¤ì˜ ì¢Œí‘œ(boxes)ì™€ í´ë˜ìŠ¤(labels), ì ìˆ˜(scores)ê°€ ì˜ˆì¸¡ë©ë‹ˆë‹¤.


```python
model.eval()
for imgs, annotations in test_data_loader:
    imgs = list(img.to(device) for img in imgs)
    annotations = [{k: v.to(device) for k, v in t.items()} for t in annotations]

    model.eval()
    pred = model(imgs)
    print(pred)
    break
```

    [{'boxes': tensor([[242.3134,  68.6447, 340.8331, 190.1095]], device='cuda:0',
           grad_fn=<StackBackward>), 'labels': tensor([1], device='cuda:0'), 'scores': tensor([0.9971], device='cuda:0', grad_fn=<IndexBackward>)}, {'boxes': tensor([[377.1269, 123.7803, 400.0000, 156.6798],
            [304.5529, 187.4064, 324.6962, 209.4973],
            [185.9639, 153.3576, 206.7260, 182.1377],
            [ 78.9946, 141.0197, 107.9893, 172.7239]], device='cuda:0',
           grad_fn=<StackBackward>), 'labels': tensor([1, 1, 1, 1], device='cuda:0'), 'scores': tensor([0.9988, 0.9969, 0.9947, 0.9900], device='cuda:0',
           grad_fn=<IndexBackward>)}, {'boxes': tensor([[ 84.3238, 167.3977, 126.9322, 221.3025]], device='cuda:0',
           grad_fn=<StackBackward>), 'labels': tensor([1], device='cuda:0'), 'scores': tensor([0.9975], device='cuda:0', grad_fn=<IndexBackward>)}, {'boxes': tensor([], device='cuda:0', size=(0, 4), grad_fn=<StackBackward>), 'labels': tensor([], device='cuda:0', dtype=torch.int64), 'scores': tensor([], device='cuda:0', grad_fn=<IndexBackward>)}]
    

ê·¸ëŸ¬ë©´ ì˜ˆì¸¡í•œ ë°”ìš´ë”© ë°•ìŠ¤ë¥¼ ê·¸ë¦¼ìœ¼ë¡œ ì¶œë ¥í•´ë³´ê² ìŠµë‹ˆë‹¤.

ì•„ë˜ `plot_image` í•¨ìˆ˜ë¥¼ ë§Œë“¤ì–´ ì´ë¯¸ì§€ ìœ„ì— ì§ì‚¬ê°í˜• ëª¨ì–‘ì˜ ë°”ìš´ë”© ë°•ìŠ¤ë¥¼ í‘œì‹œí•˜ì˜€ìŠµë‹ˆë‹¤.



```python
def plot_image(img_tensor, annotation):
    
    fig,ax = plt.subplots(1)
    img = img_tensor.cpu().data

    # Display the image
    ax.imshow(img.permute(1, 2, 0))
    
    for idx in range(len(annotation["boxes"])):
        xmin, ymin, xmax, ymax = annotation["boxes"][idx]

        if annotation['labels'][idx] == 0 :
            rect = patches.Rectangle((xmin,ymin),(xmax-xmin),(ymax-ymin),linewidth=1,edgecolor='r',facecolor='none')
        
        elif annotation['labels'][idx] == 1 :
            
            rect = patches.Rectangle((xmin,ymin),(xmax-xmin),(ymax-ymin),linewidth=1,edgecolor='g',facecolor='none')
            
        else :
        
            rect = patches.Rectangle((xmin,ymin),(xmax-xmin),(ymax-ymin),linewidth=1,edgecolor='b',facecolor='none')

        # Add the patch to the Axes
        ax.add_patch(rect)

    plt.show()
```

Predictionì´ í•™ìŠµì‹œí‚¨ ëª¨ë¸ì˜ ê²°ê³¼ì´ê³ , Targetì´ ì‹¤ì œ ë°”ìš´ë”©ë°•ìŠ¤ì…ë‹ˆë‹¤.

Targetê³¼ ë˜‘ê°™ì´ ë§ˆìŠ¤í¬ë¥¼ ì“´ ì‚¬ëŒ í•œ ëª…ì„ ì œëŒ€ë¡œ ì˜ˆì¸¡í•˜ê³  ìˆìŠµë‹ˆë‹¤.


```python
print("Prediction : ", pred[0]['labels'])
plot_image(imgs[0], pred[0])
print("Target : ", annotations[0]['labels'])
plot_image(imgs[0], annotations[0])
```

    Prediction :  tensor([1], device='cuda:0')
    


![png](output_34_1.png)


    Target :  tensor([1], device='cuda:0')
    


![png](output_34_3.png)


ì´ë²ˆì—” ì „ì²´ ì‹œí—˜ ë°ì´í„°ì— ëŒ€í•´ì„œ ì˜ˆì¸¡ ê²°ê³¼ë¥¼ í‰ê°€í•´ë³´ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤. ì˜ˆì¸¡ í‰ê°€ ì§€í‘œë¡œëŠ” mAPë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.


```python
def get_batch_statistics(outputs, targets, iou_threshold):
    """ Compute true positives, predicted scores and predicted labels per sample """
    batch_metrics = []
    for sample_i in range(len(outputs)):

        if outputs[sample_i] is None:
            continue

        output = outputs[sample_i] # predict
        pred_boxes = output['boxes']
        pred_scores = output['scores']
        pred_labels = output['labels']

        true_positives = torch.zeros(pred_boxes.shape[0])   # ì˜ˆì¸¡ ê°ì²´ ê°œìˆ˜
 
        annotations = targets[sample_i]  # actual
        target_labels = annotations['labels'] if len(annotations) else []
        if len(annotations):    # len(annotations) = 3
            detected_boxes = []
            target_boxes = annotations['boxes']

            for pred_i, (pred_box, pred_label) in enumerate(zip(pred_boxes, pred_labels)):

                # If targets are found break
                if len(detected_boxes) == len(annotations): 
                    break

                # Ignore if label is not one of the target labels
                if pred_label not in target_labels:
                    continue
                iou, box_index = bbox_iou(pred_box.unsqueeze(0), target_boxes).max(0)   # box_index : ì‹¤ì œ ì–´ë–¤ ë°”ìš´ë”© ë°•ìŠ¤ë‘ IoU ê°€ ê°€ì¥ ë†’ì€ì§€ index
                if iou >= iou_threshold and box_index not in detected_boxes:
                    true_positives[pred_i] = 1
                    detected_boxes += [box_index]  # ì˜ˆì¸¡ëœê±°ë‘ ì‹¤ì œë‘ ë§¤í•‘í•´ì„œ í•˜ë‚˜ì”© index ì±„ì›€
        batch_metrics.append([true_positives, pred_scores, pred_labels])
    return batch_metrics
```


```python
def bbox_iou(box1, box2, x1y1x2y2=True):
    """
    Returns the IoU of two bounding boxes
    """
    if not x1y1x2y2:
        # Transform from center and width to exact coordinates
        b1_x1, b1_x2 = box1[:, 0] - box1[:, 2] / 2, box1[:, 0] + box1[:, 2] / 2
        b1_y1, b1_y2 = box1[:, 1] - box1[:, 3] / 2, box1[:, 1] + box1[:, 3] / 2
        b2_x1, b2_x2 = box2[:, 0] - box2[:, 2] / 2, box2[:, 0] + box2[:, 2] / 2
        b2_y1, b2_y2 = box2[:, 1] - box2[:, 3] / 2, box2[:, 1] + box2[:, 3] / 2
    else:
        # Get the coordinates of bounding boxes
        b1_x1, b1_y1, b1_x2, b1_y2 = box1[:, 0], box1[:, 1], box1[:, 2], box1[:, 3]
        b2_x1, b2_y1, b2_x2, b2_y2 = box2[:, 0], box2[:, 1], box2[:, 2], box2[:, 3]

    # get the corrdinates of the intersection rectangle
    inter_rect_x1 = torch.max(b1_x1, b2_x1)
    inter_rect_y1 = torch.max(b1_y1, b2_y1)
    inter_rect_x2 = torch.min(b1_x2, b2_x2)
    inter_rect_y2 = torch.min(b1_y2, b2_y2)
    # Intersection area
    inter_area = torch.clamp(inter_rect_x2 - inter_rect_x1 + 1, min=0) * torch.clamp(inter_rect_y2 - inter_rect_y1 + 1, min=0)
    # Union Area
    b1_area = (b1_x2 - b1_x1 + 1) * (b1_y2 - b1_y1 + 1)
    b2_area = (b2_x2 - b2_x1 + 1) * (b2_y2 - b2_y1 + 1)

    iou = inter_area / (b1_area + b2_area - inter_area + 1e-16)

    return iou
```


```python
def ap_per_class(tp, conf, pred_cls, target_cls):
    """ Compute the average precision, given the recall and precision curves.
    Source: https://github.com/rafaelpadilla/Object-Detection-Metrics.
    # Arguments
        tp:    True positives (list).
        conf:  Objectness value from 0-1 (list).
        pred_cls: Predicted object classes (list).
        target_cls: True object classes (list).
    # Returns
        The average precision as computed in py-faster-rcnn.
    """

    # Sort by objectness
    i = torch.argsort(-conf)
    tp, conf, pred_cls = tp[i], conf[i], pred_cls[i]

    # Find unique classes
    unique_classes = torch.unique(target_cls)   # 2ê°€ ê±°ì˜ ì˜ˆì¸¡ì•ˆë¨

    # Create Precision-Recall curve and compute AP for each class
    ap, p, r = [], [], []
    for c in unique_classes:
        i = pred_cls == c
        n_gt = (target_cls == c).sum()  # Number of ground truth objects
        n_p = i.sum()  # Number of predicted objects

        if n_p == 0 and n_gt == 0:
            continue
        elif n_p == 0 or n_gt == 0:
            ap.append(0)
            r.append(0)
            p.append(0)
        else:
            # Accumulate FPs and TPs
            fpc = torch.cumsum(1 - tp[i],-1)
            tpc = torch.cumsum(tp[i],-1)

            # Recall
            recall_curve = tpc / (n_gt + 1e-16)
            r.append(recall_curve[-1])

            # Precision
            precision_curve = tpc / (tpc + fpc)
            p.append(precision_curve[-1])

            # AP from recall-precision curve
            ap.append(compute_ap(recall_curve, precision_curve))

    # Compute F1 score (harmonic mean of precision and recall)
    p, r, ap = torch.tensor(np.array(p)), torch.tensor(np.array(r)), torch.tensor(np.array(ap))
    f1 = 2 * p * r / (p + r + 1e-16)

    return p, r, ap, f1, unique_classes
```


```python
def compute_ap(recall, precision):
    """ Compute the average precision, given the recall and precision curves.
    Code originally from https://github.com/rbgirshick/py-faster-rcnn.
    # Arguments
        recall:    The recall curve (list).
        precision: The precision curve (list).
    # Returns
        The average precision as computed in py-faster-rcnn.
    """
    # correct AP calculation
    # first append sentinel values at the end
    mrec = np.concatenate(([0.0], recall, [1.0]))
    mpre = np.concatenate(([0.0], precision, [0.0]))

    # compute the precision envelope
    for i in range(mpre.size - 1, 0, -1):
        mpre[i - 1] = np.maximum(mpre[i - 1], mpre[i])

    # to calculate area under PR curve, look for points
    # where X axis (recall) changes value
    i = np.where(mrec[1:] != mrec[:-1])[0]

    # and sum (\Delta recall) * prec
    ap = np.sum((mrec[i + 1] - mrec[i]) * mpre[i + 1])
    return ap
```


```python
labels = []
sample_metrics = []  # List of tuples (TP, confs, pred)
for imgs, annotations in test_data_loader:
    imgs = list(img.to(device) for img in imgs)
    for t in annotations:
      labels += t['labels']
    annotations = [{k: v.to(device) for k, v in t.items()} for t in annotations]
    
    with torch.no_grad():
      model.eval()
      pred = model(imgs)
      sample_metrics += get_batch_statistics(pred, annotations, iou_threshold=0.5)
      
# Concatenate sample statistics
true_positives, pred_scores, pred_labels = [torch.cat(x, 0) for x in list(zip(*sample_metrics))]  # ë°°ì¹˜ê°€ ì „ë¶€ í•©ì³ì§
precision, recall, AP, f1, ap_class = ap_per_class(true_positives, pred_scores, pred_labels, torch.tensor(labels))
mAP = torch.mean(AP)
print(f'mAP : {mAP}')
print(f'AP : {AP}')
```

    mAP : 0.2140198543853392
    AP : tensor([0.0000, 0.4726, 0.1695], dtype=torch.float64)
    


```python
# AP, mAP
```




    (tensor([0.0000, 0.4729, 0.2351], dtype=torch.float64),
     tensor(0.2360, dtype=torch.float64))




```python

```
