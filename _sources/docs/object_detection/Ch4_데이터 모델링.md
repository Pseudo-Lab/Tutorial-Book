# 4. ë°ì´í„° ëª¨ë¸ë§

ì´ë²ˆ ì¥ì—ì„œëŠ” torchvisionì—ì„œ ì œê³µí•˜ëŠ” one-stage ëª¨ë¸ì¸ RetinaNetì„ í™œìš©í•˜ì—¬ ì˜ë£Œìš© ë§ˆìŠ¤í¬ ê²€ì¶œ ëª¨ë¸ì„ êµ¬ì¶•í•´ë³´ê² ìŠµë‹ˆë‹¤. torchvision APIë¥¼ í™œìš©í•˜ì—¬ ì‚¬ì „ í›ˆë ¨ëœ ëª¨ë¸ì„ ê°€ì§€ê³  ì˜¨ í›„ ì „ì´ í•™ìŠµì„ í†µí•´ ëª¨ë¸ì„ êµ¬ì¶•í•  ê²ƒì…ë‹ˆë‹¤. 



```python
!pip install mapcalc
```

    Collecting mapcalc
      Downloading https://files.pythonhosted.org/packages/11/c3/8c6e3c2e3b57d87a0fca11844e0b215c75f0cb6609a32baba9b440effc62/mapcalc-0.1.1.tar.gz
    Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from mapcalc) (1.18.5)
    Building wheels for collected packages: mapcalc
      Building wheel for mapcalc (setup.py) ... [?25l[?25hdone
      Created wheel for mapcalc: filename=mapcalc-0.1.1-cp36-none-any.whl size=4358 sha256=d4f8c69bbf1af587595d9ef283bc7be30699c4cdc5ec35fa2b02e0a9bd7bf233
      Stored in directory: /root/.cache/pip/wheels/d4/cf/c7/1d7e3b774fe610e31c0611176fbbbfaead125a3611800caee5
    Successfully built mapcalc
    Installing collected packages: mapcalc
    Successfully installed mapcalc-0.1.1
    


```python
import mapcalc
```


```python
# xmin, xmax, ymin, ymax
true = {'boxes':[[0,100,0,100],
                 [200,250,0,100]],
        'labels':[0, 1]}

pred = {'boxes':[[0,100,0,100],
                 [200,250,0,100]],
        'labels':[0, 1],
        'scores':[0.9, 0.6]}
```


```python
mapcalc.calculate_map(true, pred, 0.5)
```




    0.5




```python
mapcalc.calculate_map_range(true, pred, 0.05, 0.95, 0.05)
```




    0.5




```python
from mapcalc import calculate_map, calculate_map_range

# calculates the mAP for an IOU threshold of 0.5
calculate_map(ground_truth, result, 0.5)

# calculates the mAP average for the IOU thresholds 0.05, 0.1, 0.15, ..., 0.90, 0.95.
calculate_map_range(ground_truth, result, 0.05, 0.95, 0.05)
```

## 4.1 ë°ì´í„° ë‹¤ìš´ë¡œë“œ

ëª¨ë¸ë§ ì‹¤ìŠµì„ ìœ„í•´ 2.1ì ˆì— ë‚˜ì˜¨ ì½”ë“œë¥¼ í™œìš©í•˜ì—¬ ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜¤ê² ìŠµë‹ˆë‹¤.


```python
!git clone https://github.com/Pseudo-Lab/Tutorial-Book-DataLoader
!python Tutorial-Book-DataLoader/PL_data_loader.py --data FaceMaskDetection
!unzip -q Face\ Mask\ Detection.zip
```

    Cloning into 'Tutorial-Book-DataLoader'...
    remote: Enumerating objects: 6, done.[K
    remote: Counting objects: 100% (6/6), done.[K
    remote: Compressing objects: 100% (5/5), done.[K
    remote: Total 6 (delta 0), reused 3 (delta 0), pack-reused 0[K
    Unpacking objects: 100% (6/6), done.
    Face Mask Detection.zip is done!
    

## 4.2 ë°ì´í„° ë¶„ë¦¬
3.4ì ˆì—ì„œ í™•ì¸í•œ ë°ì´í„° ë¶„ë¦¬ ë°©ë²•ì„ í™œìš©í•˜ì—¬ ë°ì´í„°ë¥¼ ë¶„ë¦¬í•˜ê² ìŠµë‹ˆë‹¤. 


```python
import os
print(len(os.listdir('annotations')))
print(len(os.listdir('images')))
```

    683
    683
    

ì´ 853ê°œì˜ ì´ë¯¸ì§€ê°€ ì¡´ì¬í•˜ëŠ” ê²ƒì„ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì¼ë°˜ì ìœ¼ë¡œ í•™ìŠµ ë°ì´í„°ì™€ ì‹œí—˜ ë°ì´í„°ì˜ ë¹„ìœ¨ì€ 7:3 ìœ¼ë¡œ ê°€ì ¸ê°‘ë‹ˆë‹¤. ì´ë²ˆ ë°ì´í„°ëŠ” ì „ì²´ ë°ì´í„°ì…‹ ê°œìˆ˜ê°€ ì ìœ¼ë¯€ë¡œ 8:2 ë¹„ìœ¨ì„ ê°€ì ¸ê°€ë³´ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤. 853ê°œì˜ ë°ì´í„° ì¤‘ 170ê°œë¥¼ ì‹œí—˜ ë°ì´í„°ë¡œ ì‚¬ìš©í•˜ê¸° ìœ„í•´ í•´ë‹¹ ë°ì´í„°ë¥¼ ë³„ë„ì˜ í´ë”ë¡œ ì˜®ê²¨ ì£¼ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤. ìš°ì„  ë¦¬ëˆ…ìŠ¤ ëª…ë ¹ì–´ mkdirë¥¼ í™œìš©í•˜ì—¬ ì‹œí—˜ ë°ì´í„°ë¥¼ ë‹´ì„ í´ë”ë¥¼ ìƒì„±í•©ë‹ˆë‹¤. 


```python
!mkdir test_images
!mkdir test_annotations
```

ìœ„ ì½”ë“œë¥¼ ì‹¤í–‰ í•˜ë©´ ê·¸ë¦¼ 4.2ì™€ ê°™ì´ test_images í´ë”ì™€ test_annotations í´ë”ê°€ ìƒì„±ëœ ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ì œ images í´ë”ì™€ annotations í´ë”ì— ìˆëŠ” íŒŒì¼ ê°ê° 170ê°œì”©ì„ ìƒˆë¡œ ìƒì„±í•œ í´ë”ë¡œ ì˜®ê¸°ê² ìŠµë‹ˆë‹¤. random ëª¨ë“ˆì— ìˆëŠ” sample í•¨ìˆ˜ë¥¼ í™œìš©í•´ ë¬´ì‘ìœ„ë¡œ ìˆ«ìë¥¼ ì¶”ì¶œí•œ í›„ ì¸ë±ìŠ¤ ê°’ìœ¼ë¡œ í™œìš©í•˜ê² ìŠµë‹ˆë‹¤. 


```python
import random
random.seed(1234)
idx = random.sample(range(853), 170)
print(len(idx))
print(idx[:10])
```

    170
    [796, 451, 119, 7, 92, 826, 596, 35, 687, 709]
    


```python
import numpy as np
import shutil

for img in np.array(sorted(os.listdir('images')))[idx]:
    shutil.move('images/'+img, 'test_images/'+img)

for annot in np.array(sorted(os.listdir('annotations')))[idx]:
    shutil.move('annotations/'+annot, 'test_annotations/'+annot)


```

ìœ„ì˜ ì½”ë“œ ì²˜ëŸ¼ shutil íŒ¨í‚¤ì§€ë¥¼ í™œìš©í•´ 170ê°œì˜ ì´ë¯¸ì§€ì™€ 170ê°œì˜ ì¢Œí‘œ íŒŒì¼ë“¤ì„ ê°ê° test_imagesí´ë”ì™€ test_annotations í´ë”ë¡œ ì˜®ê¸¸ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ê° í´ë”ë³„ íŒŒì¼ ê°œìˆ˜ë¥¼ í™•ì¸í•˜ë©´ ì•„ë˜ì™€ ê°™ì´ ë‚˜ì˜¬ ê²ƒì…ë‹ˆë‹¤. 


```python
print(len(os.listdir('annotations')))
print(len(os.listdir('images')))
print(len(os.listdir('test_annotations')))
print(len(os.listdir('test_images')))
```

    683
    683
    170
    170
    

## 4.3 ë°ì´í„°ì…‹ í´ë˜ìŠ¤ ì •ì˜
íŒŒì´í† ì¹˜ ëª¨ë¸ì„ í•™ìŠµì‹œí‚¤ê¸° ìœ„í•´ì„  ë°ì´í„°ì…‹ í´ë˜ìŠ¤ë¥¼ ì •ì˜í•´ì•¼ í•©ë‹ˆë‹¤. torchvisionì—ì„œ ì œê³µí•˜ëŠ” ê°ì²´ íƒì§€ ëª¨ë¸ì„ í•™ìŠµì‹œí‚¤ê¸° ìœ„í•œ ë°ì´í„°ì…‹ í´ë˜ìŠ¤ì˜ `__getitem__` ë©”ì„œë“œëŠ” ì´ë¯¸ì§€ íŒŒì¼ê³¼ ë°”ìš´ë”© ë°•ìŠ¤ ì¢Œí‘œë¥¼ ë°˜í™˜í•´ì•¼ í•©ë‹ˆë‹¤. ë°ì´í„°ì…‹ í´ë˜ìŠ¤ë¥¼ ì•„ë˜ ì½”ë“œë¥¼ í†µí•´ ì •ì˜ í•˜ê² ìŠµë‹ˆë‹¤. (ë¶€ê°€ ì„¤ëª… í•„ìš”, getitems ì— ëŒ€í•œ ì„¤ëª… ë”, pytorchê³µì‹ë¬¸ì„œ ì°¸ê³ )


```python
!git clone https://github.com/Paperspace/DataAugmentationForObjectDetection
import os
os.chdir('DataAugmentationForObjectDetection')
from data_aug.data_aug import *
from data_aug.bbox_util import *
import numpy as np 
import cv2 
import matplotlib.pyplot as plt 
import pickle as pkl
os.chdir('../')
```


```python
# import numpy as np 
# import pandas as pd 
# from bs4 import BeautifulSoup
# import torchvision
# from torchvision import transforms, datasets, models
# import torch
# from PIL import Image
# import matplotlib.patches as patches
# import matplotlib.pyplot as plt
# import os


# def generate_box(obj):
    
#     xmin = int(obj.find('xmin').text)
#     ymin = int(obj.find('ymin').text)
#     xmax = int(obj.find('xmax').text)
#     ymax = int(obj.find('ymax').text)
    
#     return [xmin, ymin, xmax, ymax]

# def generate_label(obj):
#     if obj.find('name').text == "with_mask":
#         return 1
#     elif obj.find('name').text == "mask_weared_incorrect":
#         return 2
#     return 0

# def generate_target(image_id, file): 
#     with open(file) as f:
#         data = f.read()
#         soup = BeautifulSoup(data, 'html.parser')
#         objects = soup.find_all('object')

#         num_objs = len(objects)

       
#         boxes = []
#         labels = []
#         for i in objects:
#             boxes.append(generate_box(i))
#             labels.append(generate_label(i))
#         boxes = torch.as_tensor(boxes, dtype=torch.float32)
#         labels = torch.as_tensor(labels, dtype=torch.int64)
#         # Tensorise img_id
#         img_id = torch.tensor([image_id])
#         # Annotation is in dictionary format
#         target = {}
#         target["boxes"] = boxes
#         target["labels"] = labels
#         target["image_id"] = img_id
        
#         return target


# class MaskDataset(object):
#     def __init__(self, transforms, path):
#         '''
#         path: path to train folder or test folder
#         '''
#         # transform moduleê³¼ img path ê²½ë¡œë¥¼ ì •ì˜
#         self.transforms = transforms
#         # load all image files, sorting them to
#         # ensure that they are aligned
#         self.path = path
#         self.imgs = list(sorted(os.listdir(self.path)))


#     def __getitem__(self, idx): #special method
#         # load images ad masks
#         file_image = self.imgs[idx]
#         file_label = self.imgs[idx][:-3] + 'xml'
#         img_path = os.path.join(self.path, file_image)
        
#         if 'test' in self.path:
#             label_path = os.path.join("test_annotations/", file_label)
#         else:
#             label_path = os.path.join("annotations/", file_label)

#         img = Image.open(img_path).convert("RGB")
#         #Generate Label
#         target = generate_target(idx, label_path)
        
#         if self.transforms is not None:
#             img = self.transforms(img)

#         return img, target

#     def __len__(self): # len() ì ìš©ê°€ëŠ¥ì¼€ í•¨, special method
#         return len(self.imgs)
import os
import glob
import matplotlib.pyplot as plt
import matplotlib.image as mpimg
import matplotlib.patches as patches
from bs4 import BeautifulSoup
from PIL import Image
import cv2
import numpy as np
import time
import torch
import torchvision
from torch.utils.data import Dataset
from torchvision import transforms
from matplotlib import pyplot as plt
import os

def generate_box(obj):
    
    xmin = float(obj.find('xmin').text)
    ymin = float(obj.find('ymin').text)
    xmax = float(obj.find('xmax').text)
    ymax = float(obj.find('ymax').text)
    
    return [xmin, ymin, xmax, ymax]

def generate_label(obj):
    if obj.find('name').text == "with_mask":
        return 1
    elif obj.find('name').text == "mask_weared_incorrect":
        return 2
    return 0

def generate_target(file): 
    with open(file) as f:
        data = f.read()
        soup = BeautifulSoup(data, "html.parser")
        objects = soup.find_all("object")

        num_objs = len(objects)

        boxes = []
        labels = []
        for i in objects:
            boxes.append(generate_box(i))
            labels.append(generate_label(i))

        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        labels = torch.as_tensor(labels, dtype=torch.int64)
        
        target = {}
        target["boxes"] = boxes
        target["labels"] = labels
        
        return target

def plot_image_from_output(img_path, annotation):
    
    img = img_path.cpu().permute(1,2,0)
    
    fig,ax = plt.subplots(1)
    ax.imshow(img)

    
    for idx in range(len(annotation["boxes"])):
        xmin, ymin, xmax, ymax = annotation["boxes"][idx]

        if annotation['labels'][idx] == 0 :
            rect = patches.Rectangle((xmin,ymin),(xmax-xmin),(ymax-ymin),linewidth=1,edgecolor='r',facecolor='none')
        
        elif annotation['labels'][idx] == 1 :
            
            rect = patches.Rectangle((xmin,ymin),(xmax-xmin),(ymax-ymin),linewidth=1,edgecolor='g',facecolor='none')
            
        else :
        
            rect = patches.Rectangle((xmin,ymin),(xmax-xmin),(ymax-ymin),linewidth=1,edgecolor='b',facecolor='none')

        # Add the patch to the Axes
        ax.add_patch(rect)

    plt.show()

class MaskDataset(Dataset):
    def __init__(self, path, transform=None):
        self.path = path
        self.imgs = list(sorted(os.listdir(self.path)))
        self.transform = transform
        
    def __len__(self):
        return len(self.imgs)

    def __getitem__(self, idx):
        file_image = self.imgs[idx]
        file_label = self.imgs[idx][:-3] + 'xml'
        img_path = os.path.join(self.path, file_image)
        
        if 'test' in self.path:
            label_path = os.path.join("test_annotations/", file_label)
        else:
            label_path = os.path.join("annotations/", file_label)

        # ì´ë¯¸ì§€ ì½ì–´ì˜¤ê¸°, ë°”ìš´ë”© ë°•ìŠ¤ ì½ì–´ì˜¤ê¸°
        img = Image.open(img_path).convert("RGB")
        target = generate_target(label_path)
        
        # augmentation ì‹¤ì‹œ
        to_tensor = torchvision.transforms.ToTensor()

        if self.transform:
            img, transform_target = self.transform(np.array(img), np.array(target['boxes']))
            target['boxes'] = torch.as_tensor(transform_target)

        # tensorë¡œ ë³€ê²½
        img = to_tensor(img)


        return img, target

data_transform = Sequence([RandomHorizontalFlip(0.5), RandomRotate(180)])

def collate_fn(batch):
    return tuple(zip(*batch))

dataset = MaskDataset('images/', transform = data_transform)
test_dataset = MaskDataset('test_images/')

data_loader = torch.utils.data.DataLoader(dataset, batch_size=4, collate_fn=collate_fn)
test_data_loader = torch.utils.data.DataLoader(test_dataset, batch_size=4, collate_fn=collate_fn)
```


```python
for im, ta in data_loader:
    print(im, ta)
    break
```

    (tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
             [0., 0., 0.,  ..., 0., 0., 0.],
             [0., 0., 0.,  ..., 0., 0., 0.],
             ...,
             [0., 0., 0.,  ..., 0., 0., 0.],
             [0., 0., 0.,  ..., 0., 0., 0.],
             [0., 0., 0.,  ..., 0., 0., 0.]],
    
            [[0., 0., 0.,  ..., 0., 0., 0.],
             [0., 0., 0.,  ..., 0., 0., 0.],
             [0., 0., 0.,  ..., 0., 0., 0.],
             ...,
             [0., 0., 0.,  ..., 0., 0., 0.],
             [0., 0., 0.,  ..., 0., 0., 0.],
             [0., 0., 0.,  ..., 0., 0., 0.]],
    
            [[0., 0., 0.,  ..., 0., 0., 0.],
             [0., 0., 0.,  ..., 0., 0., 0.],
             [0., 0., 0.,  ..., 0., 0., 0.],
             ...,
             [0., 0., 0.,  ..., 0., 0., 0.],
             [0., 0., 0.,  ..., 0., 0., 0.],
             [0., 0., 0.,  ..., 0., 0., 0.]]]),) ({'boxes': tensor([[358.5566, 225.6981, 399.4153, 252.4607],
            [298.2506, 168.8346, 349.5203, 203.1705],
            [232.8646, 102.2513, 285.9851, 136.1460]]), 'labels': tensor([0, 1, 0])},)
    

ìµœì¢…ì ìœ¼ë¡œ í›ˆë ¨ìš© ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜¬ data_loaderì™€ ì‹œí—˜ ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜¤ test_data_loaderë¥¼ ê°ê° ì •ì˜í•©ë‹ˆë‹¤. 

## 4.4 ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°

torchvisionì—ì„œëŠ” ê°ì¢… ì»´í“¨í„° ë¹„ì „ì„ ìœ„í•œ ëª¨ë¸ì„ ì‰½ê²Œ ì‚¬ìš©í•  ìˆ˜ëŠ” ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤. ê°ì²´ íƒì§€ ëª¨ë¸ë„ ì‰½ê²Œ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” APIë¥¼ ì œê³µí•©ë‹ˆë‹¤. (ì„¤ëª… ë¶€ìì—° ìŠ¤ëŸ¬ì›€ ê³ ì³ì•¼í•¨)

torchvision.models ëª¨ë“ˆì„ í™œìš©í•˜ì—¬ RetinaNet ëª¨ë¸ì„ ë¶ˆëŸ¬ì˜¤ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤. RetinaNetì€ torchvision 0.8.0 ì´ìƒì—ì„œ ì œê³µë˜ë¯€ë¡œ, ì•„ë˜ ì½”ë“œë¥¼ í™œìš©í•˜ì—¬ torchvision ë²„ì „ì„ ë§ì¶°ì¤ë‹ˆë‹¤.


```python
!pip install torch==1.7.0+cu101 torchvision==0.8.1+cu101 torchaudio==0.7.0 -f https://download.pytorch.org/whl/torch_stable.html
```

    Looking in links: https://download.pytorch.org/whl/torch_stable.html
    Requirement already satisfied: torch==1.7.0+cu101 in /usr/local/lib/python3.6/dist-packages (1.7.0+cu101)
    Requirement already satisfied: torchvision==0.8.1+cu101 in /usr/local/lib/python3.6/dist-packages (0.8.1+cu101)
    Requirement already satisfied: torchaudio==0.7.0 in /usr/local/lib/python3.6/dist-packages (0.7.0)
    Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch==1.7.0+cu101) (0.7)
    Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch==1.7.0+cu101) (1.18.5)
    Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch==1.7.0+cu101) (0.16.0)
    Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch==1.7.0+cu101) (3.7.4.3)
    Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision==0.8.1+cu101) (7.0.0)
    


```python
import torchvision
import torch
```


```python
torchvision.__version__
```




    '0.8.1+cu101'



torchvision ë²„ì „ì´ 0.8.0 ì´ìƒì„ì„ í™•ì¸í–ˆìœ¼ë©´ ì•„ë˜ ì½”ë“œë¥¼ ì‹¤í–‰í•˜ì—¬ RetinaNet ëª¨ë¸ì„ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤. Face Mask Detection ë°ì´í„°ì…‹ì— 3ê°œì˜ í´ë˜ìŠ¤ê°€ ì¡´ì¬í•˜ë¯€ë¡œ num_classes ë§¤ê°œë³€ìˆ˜ë¥¼ 3ìœ¼ë¡œ ì •ì˜í•˜ê³ , ì „ì´ í•™ìŠµì„ í•  ê²ƒì´ê¸° ë•Œë¬¸ì— backbone êµ¬ì¡°ëŠ” ì‚¬ì „ í•™ìŠµ ëœ ê°€ì¤‘ì¹˜ë¥¼, ê·¸ ì™¸ ê°€ì¤‘ì¹˜ëŠ” ì´ˆê¸°í™” ìƒíƒœë¡œ ê°€ì ¸ì˜µë‹ˆë‹¤.

COCO ë°ì´í„°ì…‹ì— pretrain ëœê²ƒ ì„¤ëª… ì¶”ê°€


```python
retina = torchvision.models.detection.retinanet_resnet50_fpn(num_classes = 3, pretrained=False, pretrained_backbone = True)
```

## 4.5 ì „ì´ í•™ìŠµ

ëª¨ë¸ì„ ë¶ˆëŸ¬ì™”ìœ¼ë©´ ì•„ë˜ ì½”ë“œë¥¼ í™œìš©í•˜ì—¬ ì „ì´ í•™ìŠµì„ ì§„í–‰í•©ë‹ˆë‹¤. (ì½”ë“œ ì…€ ë‚˜ëˆ ì„œ ì„¤ëª… ë” ì¶”ê°€í•´ì•¼í•¨)


```python
#30
device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')

num_epochs = 10
retina.to(device)
    
# parameters
params = [p for p in retina.parameters() if p.requires_grad] # gradient calculationì´ í•„ìš”í•œ paramsë§Œ ì¶”ì¶œ
optimizer = torch.optim.SGD(params, lr=0.005,
                                momentum=0.9, weight_decay=0.0005)

len_dataloader = len(data_loader)

for epoch in range(num_epochs):
    start = time.time()
    retina.train()

    i = 0    
    epoch_loss = 0
    for images, targets in data_loader:
        images = list(image.to(device) for image in images)
        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

        loss_dict = retina(images, targets) #loss dictì— ë­ê°€ ë“¤ì–´ìˆì§€?

        losses = sum(loss for loss in loss_dict.values()) #lossesì— ë­ê°€ ë‚˜ì˜¤ì§€?

        i += 1

        optimizer.zero_grad()
        losses.backward()
        optimizer.step()
        
        # print(f'Iteration: {i}/{len_dataloader}, Loss: {losses}')
        epoch_loss += losses # lossesê°€ ìŒìˆ˜ì¸ê°€? ì™œ ë”í•˜ê¸°ë§Œ í•˜ëŠ”ë° ì¤„ì–´ë“¤ì§€?
    print(epoch_loss, f'time: {time.time() - start}')
```

    /usr/local/lib/python3.6/dist-packages/torch/nn/_reduction.py:44: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.
      warnings.warn(warning.format(ret))
    

    tensor(224.9009, device='cuda:0', grad_fn=<AddBackward0>) time: 254.03568530082703
    tensor(191.4562, device='cuda:0', grad_fn=<AddBackward0>) time: 253.98327660560608
    tensor(164.8390, device='cuda:0', grad_fn=<AddBackward0>) time: 254.9784061908722
    tensor(149.7843, device='cuda:0', grad_fn=<AddBackward0>) time: 254.7991189956665
    tensor(130.8039, device='cuda:0', grad_fn=<AddBackward0>) time: 255.5072455406189
    tensor(124.4445, device='cuda:0', grad_fn=<AddBackward0>) time: 255.77002906799316
    tensor(114.7806, device='cuda:0', grad_fn=<AddBackward0>) time: 255.5262041091919
    tensor(109.8518, device='cuda:0', grad_fn=<AddBackward0>) time: 255.75651693344116
    tensor(103.2660, device='cuda:0', grad_fn=<AddBackward0>) time: 255.81378293037415
    tensor(99.3959, device='cuda:0', grad_fn=<AddBackward0>) time: 255.89959406852722
    

ëª¨ë¸ ì¬ì‚¬ìš©ì„ ìœ„í•´ ì•„ë˜ ì½”ë“œë¥¼ ì‹¤í–‰í•˜ì—¬ í•™ìŠµëœ ê°€ì¤‘ì¹˜ë¥¼ ì €ì¥í•´ì¤ë‹ˆë‹¤. 


```python
torch.save(retina.state_dict(),f'retina_{num_epochs}.pt')
```

## 4.6 ì˜ˆì¸¡

í›ˆë ¨ì´ ë§ˆë¬´ë¦¬ ë˜ì—ˆìœ¼ë©´, ì˜ˆì¸¡ ê²°ê³¼ë¥¼ í™•ì¸í•˜ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤. ê²°ê³¼ ì‹œê°í™”ë¥¼ ìœ„í•œ ê°ì¢… í•¨ìˆ˜ë¥¼ ì •ì˜í•´ì¤ë‹ˆë‹¤. 2ì¥ ë°ì´í„° íƒìƒ‰ì—ì„œ í™œìš©í•œ í•¨ìˆ˜ë“¤ì„ ë‹¤ì‹œ ì •ì˜ í•˜ê² ìŠµë‹ˆë‹¤.


```python
import matplotlib.image as mpimg
def generate_box(obj):
    
    xmin = int(obj.find('xmin').text)
    ymin = int(obj.find('ymin').text)
    xmax = int(obj.find('xmax').text)
    ymax = int(obj.find('ymax').text)
    
    return [xmin, ymin, xmax, ymax]

def generate_label(obj):
    if obj.find('name').text == "with_mask":
        return 1
    elif obj.find('name').text == "mask_weared_incorrect":
        return 2
    return 0

def generate_target(file): 
    with open(file) as f:
        data = f.read()
        soup = BeautifulSoup(data, "html.parser")
        objects = soup.find_all("object")

        num_objs = len(objects)

        # Bounding boxes for objects
        # In coco format, bbox = [xmin, ymin, width, height]
        # In pytorch, the input should be [xmin, ymin, xmax, ymax]
        boxes = []
        labels = []
        for i in objects:
            boxes.append(generate_box(i))
            labels.append(generate_label(i))
        
        target = {}
        target["boxes"] = boxes
        target["labels"] = labels
        
        return target

def plot_image(img_path, annotation):
    
    # img = mpimg.imread(img_path)
    img = img_path.cpu().data.permute(1,2,0)
    
    fig,ax = plt.subplots(1)
    ax.imshow(img)

    
    for idx in range(len(annotation["boxes"])):
        xmin, ymin, xmax, ymax = annotation["boxes"][idx]

        if annotation['labels'][idx] == 0 :
            rect = patches.Rectangle((xmin,ymin),(xmax-xmin),(ymax-ymin),linewidth=1,edgecolor='r',facecolor='none')
        
        elif annotation['labels'][idx] == 1 :
            
            rect = patches.Rectangle((xmin,ymin),(xmax-xmin),(ymax-ymin),linewidth=1,edgecolor='g',facecolor='none')
            
        else :
        
            rect = patches.Rectangle((xmin,ymin),(xmax-xmin),(ymax-ymin),linewidth=1,edgecolor='b',facecolor='none')

        # Add the patch to the Axes
        ax.add_patch(rect)

    plt.show()
```

test_data_loaderì—ì„œ ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì™€ ëª¨ë¸ì— ë„£ì–´ í•™ìŠµ í›„, ì˜ˆì¸¡ëœ ê²°ê³¼ì™€ ì‹¤ì œ ê°’ì„ ê°ê° ì‹œê°í™” í•´ë³´ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤.


```python
for test_img, test_annot in test_data_loader:
    test = list(img.to(device) for img in test_img)
    test_ann = test_annot
    break
```


```python
id = 0
plot_image_from_output(test[id], test_ann[id])
```


![png](output_37_0.png)



```python
retina.eval()
preds = retina(test)
```


```python
_annot = {'boxes':[preds[id]['boxes'][idx] for idx, score in enumerate(preds[id]['scores']) if score > 0.5],
 'labels':[preds[id]['labels'][idx] for idx, score in enumerate(preds[id]['scores']) if score > 0.5],
 'scores':[preds[id]['scores'][idx] for idx, score in enumerate(preds[id]['scores']) if score > 0.5]}
```


```python
plot_image_from_output(test[id], _annot)
```


![png](output_40_0.png)



```python

```
