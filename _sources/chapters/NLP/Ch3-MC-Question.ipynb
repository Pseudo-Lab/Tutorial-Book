{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Ch3-MC-Question.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"XaVYtROt9-MM"},"source":["# 3. MC 문제"]},{"cell_type":"markdown","metadata":{"id":"6Jj1nr5z_DFg"},"source":["[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Pseudo-Lab/Tutorial-Book/blob/master/book/chapters/NLP/Ch3-MC-Question.ipynb)"]},{"cell_type":"markdown","metadata":{"id":"ZP6aKm_G_DDB"},"source":["이전 장에서는 실습에 사용할 데이터셋을 다운로드하고 시각화하며 전처리를 진행해보았습니다. 이번 장에서는 해당 데이터셋을 이용하여 MC 문제를 생성하는 모델을 실습해보도록 하겠습니다.\n","    \n","    \n","3.1절에서는 문제를 생성하는 데에 사용되는 GPT-2와 BERT 모델을 소개하고, 3.2절에서는 데이터를 불러와서 다시 한 번 살펴봅니다. 이어 3.3절에서는 T5 모델을 이용하여 Text 데이터를 요약하는 작업을 진행하며, 마지막으로 3.4절에서는 텍스트 속의 문장들을 필터링하고 GPT-2와 BERT를 이용하여 MC 문제를 생성해보도록 하겠습니다.\n","\n","MC 문제를 생성하는 과정은 크게 5단계로 설명될 수 있습니다. 가장 먼저 MC 문제를 생성하기 위한 지문이 되는 텍스트로부터 주요 문장들을 추출하여 요약합니다. 두번째, 추출된 주요 문장들을 유사 어휘와 어구를 활용하여 변환하며(pharaphrasing) 세번째, 이 변환된 문장들을 파싱합니다. 네번째로 이 파싱된 문장들과 GPT-2 모델을 이용해 거짓 문장을 생성하고, 마지막으로 유사도를 평가해서 정답 문장과 가장 유사하지 않은 문장들을 오답 선택지로 활용합니다. 이 과정은 3.2절부터 순서대로 확인할 수 있습니다."]},{"cell_type":"markdown","metadata":{"id":"4PU-xpEh_DAg"},"source":["## 3.1 GPT2 & BERT"]},{"cell_type":"markdown","metadata":{"id":"rg4Fjl0F_C0f"},"source":["### 3.1.1 GPT2"]},{"cell_type":"markdown","metadata":{"id":"tbZTBzVsScvj"},"source":["GPT-2는 OpenAI에서 개발한 GPT-N 시리즈의 2번째 자연어처리 모델입니다. GPT-2는 8백만 개의 웹페이지 데이터셋과 15억 개의 파라미터로부터 학습된 트랜스포머 기반 자연어처리 모델이며 이전의 단어들을 포함하는 텍스트 다음에 올 단어를 예측하는 것을 목적으로 짜여진 모델입니다. 우리는 MC 문제 생성 태스크에서 거짓 문장을 생성하는 데에 GPT-2를 활용하게 됩니다."]},{"cell_type":"markdown","metadata":{"id":"VAEw0F-f_S1r"},"source":["### 3.1.2 BERT"]},{"cell_type":"markdown","metadata":{"id":"S1n-MWxl_Sze"},"source":["BERT는 Bidirectional Encoder Representations from Transformers 의 약어로 구글에서 2018년 개발한 자연어처리 모델입니다. BERT는 Transformer를 기반으로 Sentence Embedding 혹은 Contextual Word Embedding을 구하는 네트워크로, 문장을 토큰 단위로 쪼개서 네트워크에 넣으면 전체 문장에 대한 vector와 문장 안의 단어 각각에 대응되는 vector를 출력합니다. 이를 기반으로 Text Classification 등의 Task를 학습하여 수행할 수 있습니다. "]},{"cell_type":"markdown","metadata":{"id":"ZzKRjEj9_SxB"},"source":["![](https://github.com/Pseudo-Lab/Tutorial-Book/blob/master/book/pics/NLP-ch1img02.png?raw=true)\n","\n"," - 그림 3.1 전반적인 BERT의 pre-training 과정과 fine-tuning 과정 (출처: BERT: Pre-training of Deep Bidirectional Transformers for\n","Language Understanding)"]},{"cell_type":"markdown","metadata":{"id":"h9hA3EFS_SuV"},"source":["NLP Task 성능 평가와 관련하여 다양한 NLP Task들의 성능을 바탕으로 모델들의 순위를 매기는 GLUE Benchmark(General Language Understatnding Evaluation Benchmark)라는 collection이 있는데, BERT는 여기에서 OpenAI GPT 등의 다른 모델들을 큰 차이로 앞서며 그 당시 최고의 성능을 보여 주었습니다."]},{"cell_type":"markdown","metadata":{"id":"Qd18iK8p_eLt"},"source":["## 3.2 데이터셋 다운로드"]},{"cell_type":"markdown","metadata":{"id":"pSNbM4lg_gCU"},"source":["2장에서 나온 코드를 활용하여 데이터셋을 불러오도록 하겠습니다. 데이터셋은 올바른 문법으로 수정이 된 에세이 텍스트 데이터입니다. 이 데이터는 MC 문제를 만들 지문으로 사용됩니다. 데이터를 읽어 오기 위해서 pickle 패키지를 이용합니다."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"H6Kl3KGBcJwK","executionInfo":{"status":"ok","timestamp":1626877956988,"user_tz":-540,"elapsed":1989,"user":{"displayName":"안성진","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiCjgkN_MvtrSUHRuFvstrWm6fhi5cf7CKd2UHYAw=s64","userId":"00266029492778998652"}},"outputId":"e5e4b4e0-b4a4-452a-bef5-736c7ec1aef8"},"source":["import pickle\n","\n","!git clone https://github.com/Pseudo-Lab/Tutorial-Book-Utils\n","!python Tutorial-Book-Utils/PL_data_loader.py --data NLP-QG\n","file_name = \"CoNLL+BEA_corrected_essays.pkl\"\n","open_file = open(file_name, \"rb\")\n","data = pickle.load(open_file)\n","open_file.close()"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Cloning into 'Tutorial-Book-Utils'...\n","remote: Enumerating objects: 30, done.\u001b[K\n","remote: Counting objects: 100% (30/30), done.\u001b[K\n","remote: Compressing objects: 100% (24/24), done.\u001b[K\n","remote: Total 30 (delta 9), reused 18 (delta 5), pack-reused 0\u001b[K\n","Unpacking objects: 100% (30/30), done.\n","CoNLL+BEA_corrected_essays.pkl is done!\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ecP4bO0LcSyd","executionInfo":{"status":"ok","timestamp":1626877997073,"user_tz":-540,"elapsed":40089,"user":{"displayName":"안성진","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiCjgkN_MvtrSUHRuFvstrWm6fhi5cf7CKd2UHYAw=s64","userId":"00266029492778998652"}},"outputId":"cb364a32-2dc5-4d35-d33d-658469c9ee76"},"source":["!pip install -q benepar\n","!pip install -q sentence_transformers\n","\n","import requests\n","import json\n","import benepar\n","import string\n","import nltk\n","from nltk import tokenize\n","from nltk.tokenize import sent_tokenize\n","from string import punctuation\n","import re\n","from random import shuffle\n","import spacy\n","import warnings\n","import torch\n","import pandas as pd\n","import numpy as np\n","import scipy\n","torch.manual_seed(42)\n","\n","warnings.filterwarnings(action='ignore')\n","nlp = spacy.load('en')\n","\n","nltk.download('punkt')\n","\n","benepar.download('benepar_en3')\n","benepar_parser = benepar.Parser(\"benepar_en3\")"],"execution_count":2,"outputs":[{"output_type":"stream","text":["\u001b[K     |████████████████████████████████| 3.3 MB 23.1 MB/s \n","\u001b[K     |████████████████████████████████| 2.5 MB 45.7 MB/s \n","\u001b[K     |████████████████████████████████| 1.2 MB 44.4 MB/s \n","\u001b[K     |████████████████████████████████| 895 kB 43.9 MB/s \n","\u001b[?25h  Building wheel for benepar (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[K     |████████████████████████████████| 85 kB 3.6 MB/s \n","\u001b[?25h  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package benepar_en3 to /root/nltk_data...\n","[nltk_data]   Unzipping models/benepar_en3.zip.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"O5CLZSrGUNuB","executionInfo":{"status":"ok","timestamp":1626877997075,"user_tz":-540,"elapsed":29,"user":{"displayName":"안성진","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiCjgkN_MvtrSUHRuFvstrWm6fhi5cf7CKd2UHYAw=s64","userId":"00266029492778998652"}}},"source":["device = 'cuda' if torch.cuda.is_available() else 'cpu'"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"4YTVp0TNcbty","executionInfo":{"status":"ok","timestamp":1626878196493,"user_tz":-540,"elapsed":498,"user":{"displayName":"안성진","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiCjgkN_MvtrSUHRuFvstrWm6fhi5cf7CKd2UHYAw=s64","userId":"00266029492778998652"}}},"source":["def preprocess(sentences):\n","    \n","    output = []\n","    for sent in sentences:\n","        single_quotes_present = len(re.findall(r\"['][\\w\\s.:;,!?\\\\-]+[']\",sent))>0\n","        double_quotes_present = len(re.findall(r'[\"][\\w\\s.:;,!?\\\\-]+[\"]',sent))>0\n","        question_present = \"?\" in sent\n","        if single_quotes_present or double_quotes_present or question_present :\n","            continue\n","        else:\n","            output.append(sent.strip(punctuation))\n","    return output\n"],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"id":"R6x-vvSrxMjK","executionInfo":{"status":"ok","timestamp":1626879131942,"user_tz":-540,"elapsed":309,"user":{"displayName":"안성진","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiCjgkN_MvtrSUHRuFvstrWm6fhi5cf7CKd2UHYAw=s64","userId":"00266029492778998652"}}},"source":["def get_flattened(t):\n","    sent_str_final = None\n","    if t is not None:\n","        sent_str = [\" \".join(x.leaves()) for x in list(t)]\n","        sent_str_final = [\" \".join(sent_str)]\n","        sent_str_final = sent_str_final[0]\n","    return sent_str_final\n","    \n","def get_termination_portion(main_string,sub_string):\n","    combined_sub_string = sub_string.replace(\" \",\"\")\n","    main_string_list = main_string.split()\n","    last_index = len(main_string_list)\n","    for i in range(last_index):\n","        check_string_list = main_string_list[i:]\n","        check_string = \"\".join(check_string_list)\n","        check_string = check_string.replace(\" \",\"\")\n","        if check_string == combined_sub_string:\n","            return \" \".join(main_string_list[:i])\n","                     \n","    return None\n","    \n","def get_right_most_VP_or_NP(parse_tree,last_NP = None,last_VP = None):\n","    if len(parse_tree.leaves()) == 1:\n","        return get_flattened(last_NP),get_flattened(last_VP)\n","    last_subtree = parse_tree[-1]\n","    if last_subtree.label() == \"NP\":\n","        last_NP = last_subtree\n","    elif last_subtree.label() == \"VP\":\n","        last_VP = last_subtree\n","    \n","    return get_right_most_VP_or_NP(last_subtree,last_NP,last_VP)\n","\n","\n","def get_sentence_completions(key_sentences):\n","    sentence_completion_dict = {}\n","    for individual_sentence in key_sentences:\n","        sentence = individual_sentence.rstrip('?:!.,;')\n","        tree = benepar_parser.parse(sentence)\n","        last_nounphrase, last_verbphrase =  get_right_most_VP_or_NP(tree)\n","        phrases= []\n","        if last_verbphrase is not None:\n","            verbphrase_string = get_termination_portion(sentence,last_verbphrase)\n","            if verbphrase_string is not None:\n","                phrases.append(verbphrase_string)\n","        if last_nounphrase is not None:\n","            nounphrase_string = get_termination_portion(sentence,last_nounphrase)\n","            if nounphrase_string is not None:\n","                phrases.append(nounphrase_string)\n","    \n","        longest_phrase =  sorted(phrases, key=len, reverse=True)\n","        if len(longest_phrase) == 2:\n","            first_sent_len = len(longest_phrase[0].split())\n","            second_sentence_len = len(longest_phrase[1].split())\n","            if (first_sent_len - second_sentence_len) > 4:\n","                del longest_phrase[1]\n","                \n","        if len(longest_phrase)>0:\n","            sentence_completion_dict[sentence]=longest_phrase\n","\n","    return sentence_completion_dict"],"execution_count":40,"outputs":[]},{"cell_type":"code","metadata":{"id":"HYkEOTOvYAf5","executionInfo":{"status":"ok","timestamp":1626877997078,"user_tz":-540,"elapsed":15,"user":{"displayName":"안성진","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiCjgkN_MvtrSUHRuFvstrWm6fhi5cf7CKd2UHYAw=s64","userId":"00266029492778998652"}}},"source":["def sort_by_similarity(original_sentence, generated_sentences_list):\n","\n","    sentence_embeddings = bert_model.encode(generated_sentences_list)\n","\n","    queries = [original_sentence]\n","\n","    query_embeddings = bert_model.encode(queries)\n","\n","    number_top_matches = len(generated_sentences_list)\n","\n","    dissimilar_sentences = []\n","\n","\n","    for query, query_embedding in zip(queries, query_embeddings):\n","        distances = scipy.spatial.distance.cdist([query_embedding], sentence_embeddings, \"cosine\")[0]\n","\n","        results = zip(range(len(distances)), distances)\n","        results = sorted(results, key=lambda x: x[1])\n","\n","        for idx, distance in reversed(results[0:number_top_matches]):\n","            score = 1-distance\n","            # print(score)\n","            if score < 0.99:\n","                dissimilar_sentences.append(generated_sentences_list[idx].strip())\n","           \n","    sorted_dissimilar_sentences = sorted(dissimilar_sentences, key=len)\n","    \n","    return sorted_dissimilar_sentences[:2]\n","    \n","\n","def generate_sentences(partial_sentence,full_sentence):\n","    input_ids = gpt2_tokenizer.encode(partial_sentence, return_tensors='pt') # use tokenizer to encode\n","    input_ids = input_ids.to(device)\n","    maximum_length = len(partial_sentence.split())+80 \n","\n","    sample_outputs = gpt2_model.generate( \n","        input_ids, \n","        do_sample=True, \n","        max_length=maximum_length, \n","        top_p=0.90, \n","        top_k=50,   \n","        repetition_penalty  = 10.0,\n","        num_return_sequences=5\n","    )\n","    generated_sentences=[]\n","    for i, sample_output in enumerate(sample_outputs):\n","        decoded_sentences = gpt2_tokenizer.decode(sample_output, skip_special_tokens=True)\n","        decoded_sentences_list = tokenize.sent_tokenize(decoded_sentences)\n","        generated_sentences.append(decoded_sentences_list[0]) # takes the first sentence \n","        \n","    top_3_sentences = sort_by_similarity(full_sentence, generated_sentences)\n","    \n","    return top_3_sentences\n"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"RF8JSH2PM2lx"},"source":["## load models\n","from transformers import AutoModelWithLMHead, AutoTokenizer, AutoModelForSeq2SeqLM\n","import torch\n","\n","summarize_tokenizer = AutoTokenizer.from_pretrained(\"t5-small\")\n","paraphrase_tokenizer = AutoTokenizer.from_pretrained(\"Vamsi/T5_Paraphrase_Paws\") \n","gpt2_tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n","\n","summarize_model = AutoModelWithLMHead.from_pretrained(\"t5-small\")\n","paraphrase_model = AutoModelForSeq2SeqLM.from_pretrained(\"Vamsi/T5_Paraphrase_Paws\")\n","# add the EOS token as PAD token to avoid warnings\n","gpt2_model = AutoModelWithLMHead.from_pretrained(\"gpt2\", pad_token_id=gpt2_tokenizer.eos_token_id) \n","\n","summarize_model.to(device)\n","paraphrase_model.to(device)\n","gpt2_model.to(device)\n","\n","\n","from sentence_transformers import SentenceTransformer\n","bert_model = SentenceTransformer('bert-base-nli-mean-tokens')\n","\n","bert_model.to(device)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"n4nPwllcx3sh","executionInfo":{"status":"ok","timestamp":1626878099106,"user_tz":-540,"elapsed":9,"user":{"displayName":"안성진","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiCjgkN_MvtrSUHRuFvstrWm6fhi5cf7CKd2UHYAw=s64","userId":"00266029492778998652"}}},"source":["df_TFQuestions = pd.DataFrame({'id': np.zeros(20),\n","                               'passage': np.zeros(20),\n","                               'distractor_1': np.zeros(20),\n","                               'distractor_2': np.zeros(20),\n","                               'distractor_3': np.zeros(20),\n","                               'distractor_4': np.zeros(20)})"],"execution_count":9,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Jvjcm3kJR3n-"},"source":["A: raw  \n","A': paraphrased  \n","A_False: false\n","\n","\n","- distractor_1: A'\n","- distractor_2: A'_False\n","- distractor_3: B'_False\n","- distractor_4: C'_False"]},{"cell_type":"code","metadata":{"id":"KbG5E3e4Ck6n"},"source":["## main.py\n","\n","import random\n","random.seed(42)\n","\n","passage_id_list = [163,\n"," 28,\n"," 62,\n"," 57,\n"," 35,\n"," 26,\n"," 22,\n"," 151,\n"," 108,\n"," 55,\n"," 59,\n"," 129,\n"," 167,\n"," 143,\n"," 50,\n"," 161,\n"," 107,\n"," 56,\n"," 114,\n"," 71]\n","\n","for id_idx in range(20):\n","\n","    # select passage for question generation \n","    passage_id = passage_id_list[id_idx]\n","\n","    passage = data[passage_id]\n","\n","    # summarize\n","    inputs = summarize_tokenizer.encode(\"summarize: \" + passage, return_tensors=\"pt\", max_length=512)\n","\n","    inputs = inputs.to(device)\n","\n","    outputs = summarize_model.generate(inputs, max_length=300, min_length=100, length_penalty=2.0, num_beams=4, early_stopping=True)\n","\n","    extractedSentences = summarize_tokenizer.decode(outputs[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)\n","    tokenized_sentences = nltk.tokenize.sent_tokenize(extractedSentences)\n","\n","    filter_quotes_and_questions = preprocess(tokenized_sentences)\n","\n","    # paraphrase\n","\n","    paraphrased_sentences = []\n","\n","    for summary_idx in range(len(filter_quotes_and_questions)):\n","\n","        sentence = filter_quotes_and_questions[summary_idx]\n","\n","        inputs = \"paraphrase: \" + sentence + \" </s>\"\n","\n","        encoding = paraphrase_tokenizer.encode_plus(inputs, pad_to_max_length=True, return_tensors=\"pt\")\n","        input_ids, attention_masks = encoding[\"input_ids\"].to(device), encoding[\"attention_mask\"].to(device)\n","\n","        outputs = paraphrase_model.generate(\n","            input_ids=input_ids, attention_mask=attention_masks,\n","            max_length=256,\n","            do_sample=True,\n","            top_k=50,\n","            top_p=0.95,\n","            early_stopping=True,\n","            num_return_sequences=3\n","            )\n","\n","        paraphrased_sentences.append(paraphrase_tokenizer.decode(outputs[0], skip_special_tokens=True,clean_up_tokenization_spaces=True))\n","\n","        if len(paraphrased_sentences) == 3:\n","            print('3 filled')\n","            break\n","\n","        if (summary_idx == (len(filter_quotes_and_questions) - 1)) & (len(paraphrased_sentences) < 3): # 마지막인데 채워지지 않았을 경우 존재하는 paraphrased sentence 반복해서 false 문장 생성\n","            print(summary_idx)\n","            print('hit')\n","            for paraphrase_idx in range(1, 3):\n","                paraphrased_sentences.append(paraphrase_tokenizer.decode(outputs[paraphrase_idx], skip_special_tokens=True,clean_up_tokenization_spaces=True))\n","\n","\n","    sent_completion_dict = get_sentence_completions(paraphrased_sentences)\n","\n","    df_TFQuestions.loc[id_idx, 'id'] = passage_id\n","    df_TFQuestions.loc[id_idx, 'passage'] = passage\n","    df_TFQuestions.loc[id_idx, 'distractor_1'] = list(sent_completion_dict.keys())[0]\n","\n","    distractor_cnt = 2\n","\n","    for key_sentence in sent_completion_dict:\n","\n","        if distractor_cnt == 5:\n","            break\n","\n","        partial_sentences = sent_completion_dict[key_sentence]\n","        false_sentences =[]\n","        # df_TFQuestions.loc[0, 'id'] = \n","        # print_string = \"**%s) True Sentence (from the story) :**\"%(str(index))\n","        # printmd(print_string)\n","        # print (\"  \",key_sentence)\n","        \n","        false_sents = []\n","        for partial_sent in partial_sentences:\n","            \n","            for repeat in range(10):\n","                false_sents = generate_sentences(partial_sent, key_sentence)\n","                if false_sents != []:\n","                    break\n","                    \n","            false_sentences.extend(false_sents)\n","        \n","        df_TFQuestions.loc[id_idx, f'distractor_{distractor_cnt}'] = false_sentences[0]\n","        \n","        distractor_cnt += 1\n","    \n","    print(id_idx, 'complete')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rd2s04xrXLsL","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1626880184605,"user_tz":-540,"elapsed":347,"user":{"displayName":"안성진","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiCjgkN_MvtrSUHRuFvstrWm6fhi5cf7CKd2UHYAw=s64","userId":"00266029492778998652"}},"outputId":"f82069ec-bb0f-43f6-a11e-8649cae6c244"},"source":["df_TFQuestions"],"execution_count":42,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>passage</th>\n","      <th>distractor_1</th>\n","      <th>distractor_2</th>\n","      <th>distractor_3</th>\n","      <th>distractor_4</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>163.0</td>\n","      <td>The waters of the culinary seas had been calm ...</td>\n","      <td>The microwave is the source of life that most ...</td>\n","      <td>The microwave is the source of life that most ...</td>\n","      <td>It uses radiation to excite water particles in...</td>\n","      <td>The microwave cut cooking time in half, making...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>28.0</td>\n","      <td>The world is increasingly becoming flat with a...</td>\n","      <td>Social network sites provide us with many conv...</td>\n","      <td>Social network sites provide us with the tools...</td>\n","      <td>We can know her recent news without hanging ou...</td>\n","      <td>a piece of research shows that people will unc...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>62.0</td>\n","      <td>The best places for y...</td>\n","      <td>The report looks at the best places to visit f...</td>\n","      <td>The report looks at the best places to visit f...</td>\n","      <td>It is based on a survey of young people from t...</td>\n","      <td>It is based on my own opinion as a permanent r...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>57.0</td>\n","      <td>Puerquitour: A great experience for your mouth...</td>\n","      <td>The place is Tacos La Chule and there are gour...</td>\n","      <td>The place is Tacos La Chule and there are two ...</td>\n","      <td>The name of the place is Tacos La Chule, and t...</td>\n","      <td>The place is sooooo nice and the decoration an...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>35.0</td>\n","      <td>Nowadays, social media sites are commonly used...</td>\n","      <td>80% of people use social media sites to connec...</td>\n","      <td>80% of people use social media sites to connec...</td>\n","      <td>They consist of the function of a particular n...</td>\n","      <td>but there are also disadvantages that occur du...</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>26.0</td>\n","      <td>Interpersonal skills, like any other skills re...</td>\n","      <td>The growing use of social media has its benefi...</td>\n","      <td>The growing use of social media has its benefi...</td>\n","      <td>It is a good practice not to constantly add ne...</td>\n","      <td>It is a good practice not to use social media.\"</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>22.0</td>\n","      <td>Nowadays, with the advancement of technology, ...</td>\n","      <td>A known genetic risk should not be obligated t...</td>\n","      <td>A known genetic risk should not be obligated t...</td>\n","      <td>The government should set the law to protect t...</td>\n","      <td>However, a carrier of a known genetic risk can...</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>151.0</td>\n","      <td>In this century there have been many technolog...</td>\n","      <td>Television has brought other worlds into the l...</td>\n","      <td>Television has brought other worlds into the l...</td>\n","      <td>Television has the power to bring war into the...</td>\n","      <td>In the minds of most Americans, television is ...</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>108.0</td>\n","      <td>I met a friend about one week ago, and he aske...</td>\n","      <td>It's about a teen couple who are dying of canc...</td>\n","      <td>It's about a teen couple who are dying of canc...</td>\n","      <td>It's about a teen couple who are dying of canc...</td>\n","      <td>Now, I have an awful feeling about what I am d...</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>55.0</td>\n","      <td>Dear Sir or Madam,\\nI am writing to apply for ...</td>\n","      <td>Camp counselor is currently advertised on your...</td>\n","      <td>Camp counselor is currently advertised on the ...</td>\n","      <td>At this moment, I have finished the second yea...</td>\n","      <td>I am looking forward to hearing from you XYZ, ...</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>59.0</td>\n","      <td>Anna knew that it was going to be a very speci...</td>\n","      <td>She knew that it was going to be a very specia...</td>\n","      <td>She knew that it was going to be a very specia...</td>\n","      <td>She had known that she had been adopted since ...</td>\n","      <td>After her 18th birthday, she felt a sudden nee...</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>129.0</td>\n","      <td>On Britain's roads there is an ever-increasing...</td>\n","      <td>The government has started adding a fourth lan...</td>\n","      <td>The government has started adding a fourth lan...</td>\n","      <td>There appears to be an endless series of roadw...</td>\n","      <td>The inability to cope with the volume of traff...</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>167.0</td>\n","      <td>According the Lunde, 35% of homicide victims a...</td>\n","      <td>35% of the homicide victims are killed by some...</td>\n","      <td>35% of the homicide victims are killed by some...</td>\n","      <td>Today racial prejudice still exists, but less ...</td>\n","      <td>It still exists racial prejudice, but has been...</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>143.0</td>\n","      <td>\"In Vitro fertilisation\" is the fertilisation ...</td>\n","      <td>In a test tube</td>\n","      <td>In</td>\n","      <td>The egg is taken from the mother and placed in...</td>\n","      <td>There are people who are against this, saying ...</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>50.0</td>\n","      <td>Dear Mrs. Ashby, \\n\\nYesterday I was in Green ...</td>\n","      <td>I am very interested in this work and believe ...</td>\n","      <td>I am very interested in this work and believe ...</td>\n","      <td>I worked a year in London as a waiter at Hard ...</td>\n","      <td>I am also very good at dealing with people, I ...</td>\n","    </tr>\n","    <tr>\n","      <th>15</th>\n","      <td>161.0</td>\n","      <td>Computers have definitely affected peoples liv...</td>\n","      <td>Computers have had a significant impact on peo...</td>\n","      <td>Computers have had a significant impact on the...</td>\n","      <td>Without the use of a computer, I have to balan...</td>\n","      <td>I had to balance my checkbook once a month wit...</td>\n","    </tr>\n","    <tr>\n","      <th>16</th>\n","      <td>107.0</td>\n","      <td>Cricket is my passion. I love playing, watchin...</td>\n","      <td>Cricket is a team sport, which teaches us team...</td>\n","      <td>Cricket is a team sport, which teaches us team...</td>\n","      <td>It also teaches us how to overcome individual ...</td>\n","      <td>Cricket is going through a rough phase due to ...</td>\n","    </tr>\n","    <tr>\n","      <th>17</th>\n","      <td>56.0</td>\n","      <td>Well, I would like to talk about my school lif...</td>\n","      <td>I'm a electronics student from Italy, North</td>\n","      <td>I'm a electronics student from Michigan.</td>\n","      <td>A chance to be a great engineer one day, so I ...</td>\n","      <td>I am good at school, my marks prove it ; I hav...</td>\n","    </tr>\n","    <tr>\n","      <th>18</th>\n","      <td>114.0</td>\n","      <td>I have been learning English as a second langu...</td>\n","      <td>My teachers thought it was better to learn in ...</td>\n","      <td>My teachers thought it was better to learn in ...</td>\n","      <td>I had decided to take the Cambridge Advanced E...</td>\n","      <td>One year ago, I decided to take the Cambridge ...</td>\n","    </tr>\n","    <tr>\n","      <th>19</th>\n","      <td>71.0</td>\n","      <td>Glad to hear that you've been invited to att...</td>\n","      <td>You've been invited to the last round of inter...</td>\n","      <td>You've been invited to the last round of inter...</td>\n","      <td>Here are some tips on how to make sure that yo...</td>\n","      <td>First, the state's top elected officials are i...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["       id  ...                                       distractor_4\n","0   163.0  ...  The microwave cut cooking time in half, making...\n","1    28.0  ...  a piece of research shows that people will unc...\n","2    62.0  ...  It is based on my own opinion as a permanent r...\n","3    57.0  ...  The place is sooooo nice and the decoration an...\n","4    35.0  ...  but there are also disadvantages that occur du...\n","5    26.0  ...    It is a good practice not to use social media.\"\n","6    22.0  ...  However, a carrier of a known genetic risk can...\n","7   151.0  ...  In the minds of most Americans, television is ...\n","8   108.0  ...  Now, I have an awful feeling about what I am d...\n","9    55.0  ...  I am looking forward to hearing from you XYZ, ...\n","10   59.0  ...  After her 18th birthday, she felt a sudden nee...\n","11  129.0  ...  The inability to cope with the volume of traff...\n","12  167.0  ...  It still exists racial prejudice, but has been...\n","13  143.0  ...  There are people who are against this, saying ...\n","14   50.0  ...  I am also very good at dealing with people, I ...\n","15  161.0  ...  I had to balance my checkbook once a month wit...\n","16  107.0  ...  Cricket is going through a rough phase due to ...\n","17   56.0  ...  I am good at school, my marks prove it ; I hav...\n","18  114.0  ...  One year ago, I decided to take the Cambridge ...\n","19   71.0  ...  First, the state's top elected officials are i...\n","\n","[20 rows x 6 columns]"]},"metadata":{"tags":[]},"execution_count":42}]},{"cell_type":"code","metadata":{"id":"aKf4Ux4HXLQ9"},"source":["df_TFQuestions.to_csv('TFQuestions.csv', index=False)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vKxD1-dIijbj"},"source":["## 3.3 참고문헌\n","\n"]},{"cell_type":"markdown","metadata":{"id":"8WXSGRt8HKqr"},"source":["1. [Practical AI : Automatically Generate True or False questions from any content with OpenAI GPT2, Sentence BERT and Berkley Constituency parser](https://medium.com/swlh/practical-ai-automatically-generate-true-or-false-questions-from-any-content-with-openai-gpt2-9081ffe4d4c9)\n","\n","2. https://openai.com/blog/better-language-models/\n","\n","3. https://github.com/openai/gpt-2\n","\n","4. https://arxiv.org/abs/1810.04805"]}]}